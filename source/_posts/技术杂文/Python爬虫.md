---
title: Python爬虫
date: '2023-03-22 10:28:44'
updated: '2025-08-22 20:35:34'
categories:
  - 技术杂文
tags: null
cover: /images/custom-cover.jpg
recommend: true
---
# <font style="color:rgb(79, 79, 79);">一、都有哪些爬虫？</font>
+ <font style="color:rgb(77, 77, 77);">网络爬虫按照系统结构和实现技术，大致可以分为以下几种类型：</font>**<font style="color:rgb(77, 77, 77);">通用网络爬虫</font>**<font style="color:rgb(77, 77, 77);">、</font>**<font style="color:rgb(77, 77, 77);">聚焦网络爬虫</font>**<font style="color:rgb(77, 77, 77);">、</font>**<font style="color:rgb(77, 77, 77);">增量式网络爬虫</font>**<font style="color:rgb(77, 77, 77);">、</font>**<font style="color:rgb(77, 77, 77);">深层网络爬虫</font>**<font style="color:rgb(77, 77, 77);">。</font>
+ <font style="color:rgb(77, 77, 77);">实际的网络爬虫系统通常是几种爬虫技术相结合实现的。</font>

## 通用性网络爬虫
搜索引擎（Search Engine），例如传统的通用搜索引擎baidu、Yahoo和Google等，是一种大型复杂的网络爬虫，属于通用性网络爬虫的范畴。但是通用性搜索引擎存在着一定的局限性：

1. 不同领域、不同背景的用户往往具有不同的检索目的和需求，通用搜索引擎所返回的结果包含大量用户不关心的网页。
2. 通用搜索引擎的目标是尽可能大的网络覆盖率，有限的搜索引擎服务器资源与无限的网络数据资源之间的矛盾将进一步加深。
3. 万维网数据形式的丰富和网络技术的不断发展，图片、数据库、音频、视频多媒体等不同数据大量出现，通用搜索引擎往往对这些信息含量密集且具有一定结构的数据无能为力，不能很好地发现和获取。
4. 通用搜索引擎大多提供基于关键字的检索，难以支持根据语义信息提出的查询。

为了解决上述问题，定向抓取相关网页资源的聚焦爬虫应运而生。

## <font style="color:rgb(77, 77, 77);">聚焦爬虫</font>
  
<font style="color:rgb(77, 77, 77);">一个自动下载网页的程序，它根据既定的抓取目标，有选择地访问万维网上的网页与相关的链接，获取所需要的信息。与通用爬虫不同，聚焦爬虫并不追求大的覆盖，而将目标定为抓取与某一特定主题内容相关的网页，为面向主题的用户查询准备数据资源。</font>

## <font style="color:rgb(77, 77, 77);">增量式网络爬虫</font>
增量式网络爬虫是指对已下载网页采取增量式更新和只爬行新产生的或者已经发生变化网页的爬虫，它能够在一定程度上保证所爬行的页面是尽可能新的页面。



和周期性爬行和刷新页面的网络爬虫相比，增量式爬虫只会在需要的时候爬行新产生或发生更新的页面，并不重新下载没有发生变化的页面，可有效减少数据下载量，及时更新已爬行的网页，减小时间和空间上的耗费，但是增加了爬行算法的复杂度和实现难度。



例如：想获取赶集网的招聘信息，以前爬取过的数据没有必要重复爬取，只需要获取更新的招聘数据，这时候就要用到增量式爬虫。



## <font style="color:rgb(77, 77, 77);">深层网络爬虫</font>
Web页面按存在方式可以分为表层网页和深层网页。表层网页是指传统搜索引擎可以索引的页面，以超链接可以到达的静态网页为主构成的Web页面。深层网络是那些大部分内容不能通过静态链接获取的、隐藏在搜索表单后的，只有用户提交一些关键词才能获得的Web页面。



例如用户登录或者注册才能访问的页面。可以想象这样一个场景：爬取贴吧或者论坛中的数据，必须在用户登录后，有权限的情况下才能获取完整的数据。

## <font style="color:rgb(79, 79, 79);">网络爬虫的约束。（Robots协议——爬虫协议）</font>
+ <font style="color:rgb(77, 77, 77);">网站会通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。</font>
+ <font style="color:rgb(77, 77, 77);">不要一味追求速度但不考虑服务器的承受范围，如果因为你的过分压缩时间等造成服务器崩溃，那你可能就要因此付出一些代价。因为这种情况下的爬虫是可能被认定为网络攻击的。</font>
+ <font style="color:rgb(0, 0, 0);">以淘宝为例——</font>[https://www.taobao.com/robots.txt](https://www.taobao.com/robots.txt)

# <font style="color:rgb(77, 77, 77);">二、</font><font style="color:rgb(79, 79, 79);">python网络爬虫的流程。</font>
1. 获取网页
+ 基础技术：requests,urllib,selenium。
+ 进阶技术：多进程多线程抓取，登录抓取，图片ip封禁和使用服务器抓取。



2. 解析网页
+ 基础技术：re正则表达式，BeautifulSoup，lxml
+ 进阶技术：解决中文乱码



3. 存储数据
+ 基础技术：存入txt和存入csv文件
+ 进阶技术：存入MySQL数据库和MongoDB数据库

# 三、编写爬虫程序
网络爬虫爬行本质：递归方式。先获取url网页内容，检查页面，寻找另一个url，在获取url页面内容

## 3.1 遍历单个域名
“维基百科六度分隔理论”：把俩个不相干的主题用一个链条连接起来

——————“最快登顶理塘理论”



# 四、Scrapy
scrapy startproject  <name>  //在当前目录创建新的爬虫



# 六、存储数据
介绍三种主要的数据管理方式

## 媒体文件
俩种方式：

1. 只获取文件URL
+ 优点：简单快捷，降低目标服务器负载，节省空间
+ 缺点：
    - 盗链：在自己的网页或应用中内嵌其他网站的URL
    - 盗链容易被对方修改，每个网站基本都有防盗链措施
2. 直接下载文件



## 数据存储到CSV（逗号分隔值）——存储表格数据的常用文件格式
python的csv库

网页抓取的一个常用功能就是：获取HTML表格斌写入到CSV文件中

# 七、读取文档
## 文本编码：
一般正确读一个文件只需要知道他的扩展名就行

但是.txt文件由于每个编码格式不同，会出现乱码

ASCII：

开头补0+7位有效位数

UTF-8：8代表显示一个字符所需的最小位数

    - 所有以0开头的字节表示这个字符只需要一个字节
    - 多个字节表示一个UTF-8字符中，额外补齐位用于校验位以防止产生歧义
    - UTF-8为Unicode联盟给出的编码标准

ISO标准：为每种语言创建一个编码

    - 每个字符开头用0填充（同ASCII）



Python3.X默认编码为UTF-8

HTML网页中，编码格式通常包含在head里



# 八、数据清洗
脏数据
