---
title: 深度学习常见概念
date: '2024-07-13 16:10:14'
updated: '2025-08-21 16:34:19'
---
# 推理（Inference）、训练（Training）和验证（Validation）
:::info
在深度学习中，推理（Inference）、训练（Training）和验证（Validation）是三个关键概念，它们分别表示了不同的阶段和任务

:::

## 训练（Training）：
训练是深度学习模型的初始阶段，其中模型通过学习数据的过程来逐步优化自己的参数，以便能够捕获输入数据的特征并执行特定任务。在训练阶段，模型接收训练数据集（包括输入特征和相应的标签或目标值），并使用优化算法（如梯度下降）来调整模型参数，以最小化预测值与真实标签之间的差距（损失函数）。训练的目标是使模型能够从数据中学习到一般的模式，以便在以后的推理阶段中进行准确的预测。

## 推理（Inference）：
推理是在训练之后的阶段，用于使用训练好的模型进行预测或分类的过程。在推理阶段，模型接收新的、未见过的数据样本，并根据其已学习到的特征和模式，生成预测结果。推理是将模型应用于实际应用场景的过程，如图像分类、语音识别、自然语言处理等任务。

## 验证（Validation）：
验证是在训练阶段用于监控模型性能和避免过拟合的过程。在训练期间，通常会将训练数据集划分为两部分：训练集和验证集。模型使用训练集进行参数调整，然后使用验证集来评估模型在未见过的数据上的性能。这有助于检测模型是否过拟合训练数据，以及是否需要调整超参数或采取其他措施来提高模型的泛化能力。



总结起来：

训练是通过优化算法调整模型参数，使其能够从训练数据中学习特征和模式的过程。

推理是在训练后使用已训练模型进行实际预测或分类的过程。

验证是在训练期间使用验证集评估模型性能，以监控和改进模型的泛化能力。



官方文档：[学习基础知识 — mmrotate 文档](https://mmrotate.readthedocs.io/zh-cn/v0.2.0/intro.html)



# mAP，Precision，Recall，IoU
## <font style="color:rgb(79, 79, 79);">一、Precision，Recall</font>
**<font style="color:rgb(77, 77, 77);">mAP</font>**<font style="color:rgb(77, 77, 77);">是</font>[<font style="color:rgb(77, 77, 77);">目标检测模型</font>](https://so.csdn.net/so/search?q=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B&spm=1001.2101.3001.7020)<font style="color:rgb(77, 77, 77);">中常用的评价指标，它的英文全称是(</font>**<font style="color:rgb(77, 77, 77);">Mean Average Precision</font>**<font style="color:rgb(77, 77, 77);">)，翻译过来就是平均精确率的平均。</font>

<font style="color:rgb(77, 77, 77);">首先我们需要知道精确率(Precision)和</font>[<font style="color:rgb(77, 77, 77);">召回率</font>](https://so.csdn.net/so/search?q=%E5%8F%AC%E5%9B%9E%E7%8E%87&spm=1001.2101.3001.7020)<font style="color:rgb(77, 77, 77);">(Recall)，也称为查准率和查全率的定义</font>

**<font style="color:rgb(77, 77, 77);">Precision</font>**<font style="color:rgb(77, 77, 77);">衡量预测有多准确。也就是说，你的所有正向（positive）预测中预测正确的百分比。</font>

**<font style="color:rgb(77, 77, 77);">Recall</font>**<font style="color:rgb(77, 77, 77);">衡量发现所有正例的能力。 例如，你的所有正向（positive）预测中预测正确的占实际正向（positive）个数的比例。</font>

<font style="color:rgb(77, 77, 77);">下面是它们的数学定义：</font>

![](/images/ab45aa0a42fc0130484e9921508bcf1f.jpeg)

<font style="color:rgb(77, 77, 77);">如果对此不太清楚，具体介绍请看</font>[<font style="color:rgb(77, 77, 77);">精确率和召回率的介绍</font>](https://blog.csdn.net/qq_40765537/article/details/105344798)<font style="color:rgb(77, 77, 77);">。</font>

<font style="color:rgb(77, 77, 77);">接下来我们需要知道交并比</font>**<font style="color:rgb(77, 77, 77);">IoU(Intersection over union)</font>**

## 二、IoU <font style="color:rgb(79, 79, 79);">(Intersection over Union)</font>
#### **<font style="color:rgb(79, 79, 79);">1、什么是IoU(Intersection over Union)</font>**
<font style="color:rgb(77, 77, 77);">IoU是一种测量在特定数据集中检测相应物体准确度的一个标准。IoU是一个简单的测量标准，只要是在输出中得出一个预测范围(bounding boxex)的任务都可以用IoU来进行测量。为了可以使IoU用于测量任意大小形状的物体检测，我们需要：</font>

+ <font style="color:rgb(51, 51, 51);">ground-truth bounding boxes（人为在训练集图像中标出要检测物体的大概范围）</font>
+ <font style="color:rgb(51, 51, 51);">我们的算法得出的结果范围。</font>

<font style="color:rgb(77, 77, 77);">也就是说，这个标准用于</font>**<font style="color:rgb(77, 77, 77);">测量真实和预测之间的相关度</font>**<font style="color:rgb(77, 77, 77);">，</font><font style="color:#DF2A3F;">相关度越高，该值越高</font><font style="color:rgb(77, 77, 77);">。如下图所示。绿色标线是人为标记的正确结果（ground-truth），红色标线是算法预测的结果（predicted）。</font>

<font style="color:rgb(77, 77, 77);">  
</font>![](/images/176384d88eca8a9bae79a1922195632a.png)

<font style="color:rgb(77, 77, 77);"></font>

#### **<font style="color:rgb(79, 79, 79);">2、IoU的计算 </font>**
**<font style="color:rgb(77, 77, 77);">IoU是两个区域重叠的部分除以两个区域的集合部分得出的结果，通过设定的阈值，与这个IoU计算结果比较。</font>**

![](/images/80b3dea71df8cd0222e8ce5f1b28a884.png)

<font style="color:rgb(77, 77, 77);"></font>

**<font style="color:rgb(77, 77, 77);">举例如下：</font>**<font style="color:rgb(77, 77, 77);">绿色框是准确值，红色框是预测值。</font>

<font style="color:rgb(77, 77, 77);"></font>![](/images/d77f8ad3c0f74afe45ec3a574653ab59.png)

**<font style="color:rgb(34, 34, 38);">后记</font>**

**<font style="color:rgb(34, 34, 38);">IoU在FCN中称为IU，初看Fully Convolutional Networks for Semantic Segmentation论文，其中的IU概念没有能理解，其实那里的IU也就是IoU，检测物体轮廓不一定非得是方框，也可以是沿着物体的边线：</font>**

![](/images/517cafa94966e95ddb37704a971384cd.jpeg)

**<font style="color:rgb(34, 34, 38);">在实际的任务中，根据不同的任务要求来写不同具体实现的检测方法，但说白了其实都是IoU或者IU。</font>**

**<font style="color:rgb(34, 34, 38);">另外mean IU指的是不同类别识别准确度的平均值，比如一幅图中要识别三个物体，mean IU就是三个物体分别准确度加起来的平均值。</font>**

**<font style="color:rgb(34, 34, 38);">参考资料：</font>**

**<font style="color:rgb(34, 34, 38);">1、</font>**[**https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/**](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)

**<font style="color:rgb(34, 34, 38);">2、</font>**[**https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation/42874377#42874377**](https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation/42874377#42874377)

**<font style="color:rgb(34, 34, 38);">3、</font>**[**https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation/42874377#42874377**](https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation/42874377#42874377)

**<font style="color:rgb(34, 34, 38);">4、</font>**[**https://blog.csdn.net/gaoyu1253401563/article/details/86484851**](https://blog.csdn.net/gaoyu1253401563/article/details/86484851)

**<font style="color:rgb(77, 77, 77);"></font>**

## <font style="color:rgb(79, 79, 79);">三、AP(Average Precision)</font>
参考笔记：[目标检测算法的评估指标：mAP定义及计算方式_map评价指标-CSDN博客](https://blog.csdn.net/weixin_45683778/article/details/131831724)



# 模型
#### 1. O-RCNN框架
O-RCNN（Oriented R-CNN）是一种用于检测旋转目标的改进型区域卷积神经网络框架。它的结构和传统的Faster R-CNN类似，但其候选框（ROI）是旋转的，适合于处理旋转目标检测任务。

#### 2. ViT-B、ViT-L
ViT（Vision Transformer）是基于Transformer架构的图像分类模型。ViT-B和ViT-L分别表示Vision Transformer的基础版本（Base）和大版本（Large）。

+ **ViT-B**（Base）：较小的模型，通常具有12个Transformer层。
+ **ViT-L**（Large）：较大的模型，通常具有24个Transformer层。

#### 3. MAE预训练
MAE（Masked Autoencoders）是一种自监督学习方法，通过将输入图像的一部分遮蔽（masking），然后训练模型重建遮蔽部分，从而学习图像的表示。MAE预训练在大规模无标签数据上进行，之后将预训练模型迁移到具体任务上，如目标检测。

#### 4. HiViT-B
HiViT-B是一种基于Swin-Transformer的变体模型。HiViT-B在Swin-Transformer的基础上，将前两个stage用MLP（多层感知机）代替，从而改进模型性能。

+ **Swin-Transformer**：一种将Transformer应用于计算机视觉任务的新型架构，具有层级特征表示，适合于目标检测和分割任务。
+ **MLP**：多层感知机，是一种基本的神经网络结构。

### Faster R-CNN
Faster R-CNN是目标检测领域的经典模型之一，由Ren等人在2015年提出。它的基本结构分为以下几个部分：

1. **卷积神经网络（CNN）**：用于提取图像特征，常用的骨干网络包括VGG、ResNet等。
2. **区域建议网络（RPN）**：在提取的特征图上生成候选区域（Region Proposals）。RPN输出一组边界框（bounding boxes）和相应的分数，表示这些框包含目标的可能性。
3. **RoI池化（Region of Interest Pooling）**：将RPN生成的候选区域映射到特征图上，通过池化操作将每个候选区域转化为固定大小的特征向量。
4. **分类和回归**：使用全连接层将RoI池化后的特征向量进行分类和回归，输出目标类别和边界框位置的精细调整。

Faster R-CNN的创新点在于引入了RPN，使得候选区域生成与目标检测模型紧密结合，提高了检测速度和精度。

### Swin-Transformer
Swin-Transformer是Microsoft Research Asia的研究人员在2021年提出的一种适用于视觉任务的Transformer模型。其主要特点包括：

1. **层级结构**：Swin-Transformer将图像分割成不重叠的子窗口（windows），在每个窗口内进行自注意力计算，形成层级特征表示。这种层级结构类似于卷积神经网络（CNN）的多层特征提取。
2. **滑动窗口（Shifted Windows）机制**：为了捕捉跨窗口的全局信息，Swin-Transformer引入了滑动窗口机制，即在相邻的Transformer层中，窗口的位置会有所偏移（shifted），从而实现窗口之间的信息交互。
3. **适应性好**：Swin-Transformer可以适应不同分辨率的输入图像，适用于多种计算机视觉任务，包括图像分类、目标检测和语义分割。

Swin-Transformer在多个计算机视觉任务上的表现超过了传统的卷积神经网络（CNN），并且由于其层级结构和滑动窗口机制，在处理大尺寸图像时具有较高的效率和精度。

# 骨干网络（Backbone Network），检测头，neck是什么意思？
优质文章：[目标检测两个基础部分——backbone and detection head - 玻璃公主 - 博客园](https://www.cnblogs.com/boligongzhu/p/15076220.html)

#### 作用
1. **特征提取**：骨干网络负责提取图像的底层和高层特征，包括边缘、纹理、形状和对象部件等。
2. **特征表示**：通过多层卷积和池化操作，骨干网络将原始图像数据转换为高维特征向量，这些向量能够更好地表示图像中的内容。
3. **数据降维**：通过特征提取过程，骨干网络可以将高维的图像数据降维成易于处理的特征图，减少计算复杂度。

#### 常见的骨干网络
+ **ResNet（Residual Network）**：使用残差连接解决了深度神经网络训练中的梯度消失问题，常用的版本包括ResNet-50、ResNet-101等。
+ **VGGNet**：使用简单的卷积层和池化层堆叠而成，网络深度较深，常用的版本包括VGG-16、VGG-19。
+ **MobileNet**：针对移动设备和嵌入式设备设计的轻量级网络，使用深度可分离卷积减少计算量。
+ **DenseNet（Densely Connected Convolutional Networks）**：通过密集连接（Dense Connection）机制，解决了梯度消失问题，提高了特征复用性。
+ **Swin-Transformer**：基于Transformer架构，通过分割窗口和滑动窗口机制提取图像特征，适用于大尺寸图像。

#### 示例：Faster R-CNN中的骨干网络
在Faster R-CNN框架中，骨干网络位于最前端，用于提取输入图像的特征。以下是具体步骤：

1. **输入图像**：原始输入图像被送入骨干网络。
2. **特征提取**：骨干网络通过多层卷积和池化操作提取特征，输出特征图（Feature Map）。
3. **区域建议网络（RPN）**：特征图被传递给RPN，生成候选区域（Region Proposals）。
4. **ROI池化和分类回归**：候选区域通过ROI池化操作后，进行分类和边界框回归，输出目标类别和位置。

例如，使用ResNet-50作为骨干网络的Faster R-CNN：

+ **ResNet-50**：提取特征并生成特征图。
+ **RPN**：基于特征图生成候选区域。
+ **分类和回归头**：对候选区域进行进一步处理，输出检测结果。

#### 总结
骨干网络是深度学习模型的基础部分，负责将输入图像转换为高维特征表示。这些特征随后被用于更高级的任务，如目标检测、图像分类和语义分割。选择合适的骨干网络可以显著提高模型的性能和效率。



## 预训练模型权重是什么意思
预训练模型权重指的是在一个大型数据集（例如ImageNet）上预先训练好的神经网络模型的参数。这些权重文件包含了模型在预训练过程中学到的特征表示，可以用来初始化你的模型，从而加快训练过程，提高模型的性能。

具体来说，预训练模型权重有以下几个优点：

1. **加快训练速度**：由于模型已经在一个大型数据集上学习到了一些通用的特征，你在自己的数据集上进行训练时，可以从这些预训练的权重开始，而不是从随机初始化的权重开始。这大大减少了训练时间。
2. **提高性能**：预训练的模型已经在大量数据上学到了丰富的特征表示，因此在小数据集上进行微调时，可以更容易地达到更高的性能。
3. **更好的泛化能力**：预训练模型在一个大型、多样的数据集上训练，可以学习到更通用的特征，这有助于模型在新任务上的泛化。

在你的例子中，提供的权重文件（如`mae_vit_small_800e.pth`、`mae_pretrain_vit_base_full.pth`等）是基于ImageNet数据集通过MAE（Masked Autoencoders）方法预训练的。你可以下载这些权重文件并在你的模型中加载，以利用这些预训练的优势。

## 
