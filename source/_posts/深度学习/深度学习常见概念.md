---
title: 深度学习常见概念
date: '2024-07-13 16:10:14'
updated: '2025-09-04 16:36:58'
categories:
  - 人工智能
  - 技术杂文
tags:
  - 深度学习
cover: /images/custom-cover.jpg
recommend: true
---
## 推理（Inference）、训练（Training）和验证（Validation）
:::info
在深度学习中，推理（Inference）、训练（Training）和验证（Validation）是三个关键概念，它们分别表示了不同的阶段和任务

:::

### 训练（Training）：
训练是深度学习模型的初始阶段，其中模型通过学习数据的过程来逐步优化自己的参数，以便能够捕获输入数据的特征并执行特定任务。在训练阶段，模型接收训练数据集（包括输入特征和相应的标签或目标值），并使用优化算法（如梯度下降）来调整模型参数，以最小化预测值与真实标签之间的差距（损失函数）。训练的目标是使模型能够从数据中学习到一般的模式，以便在以后的推理阶段中进行准确的预测。

### 推理（Inference）：
推理是在训练之后的阶段，用于使用训练好的模型进行预测或分类的过程。在推理阶段，模型接收新的、未见过的数据样本，并根据其已学习到的特征和模式，生成预测结果。推理是将模型应用于实际应用场景的过程，如图像分类、语音识别、自然语言处理等任务。

### 验证（Validation）：
验证是在训练阶段用于监控模型性能和避免过拟合的过程。在训练期间，通常会将训练数据集划分为两部分：训练集和验证集。模型使用训练集进行参数调整，然后使用验证集来评估模型在未见过的数据上的性能。这有助于检测模型是否过拟合训练数据，以及是否需要调整超参数或采取其他措施来提高模型的泛化能力。



总结起来：

训练是通过优化算法调整模型参数，使其能够从训练数据中学习特征和模式的过程。

推理是在训练后使用已训练模型进行实际预测或分类的过程。

验证是在训练期间使用验证集评估模型性能，以监控和改进模型的泛化能力。



官方文档：[学习基础知识 — mmrotate 文档](https://mmrotate.readthedocs.io/zh-cn/v0.2.0/intro.html)



## mAP，Precision，Recall，IoU
### <font style="color:rgb(79, 79, 79);">一、Precision，Recall</font>
**<font style="color:rgb(77, 77, 77);">mAP</font>**<font style="color:rgb(77, 77, 77);">是</font>[<font style="color:rgb(77, 77, 77);">目标检测模型</font>](https://so.csdn.net/so/search?q=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B&spm=1001.2101.3001.7020)<font style="color:rgb(77, 77, 77);">中常用的评价指标，它的英文全称是(</font>**<font style="color:rgb(77, 77, 77);">Mean Average Precision</font>**<font style="color:rgb(77, 77, 77);">)，翻译过来就是平均精确率的平均。</font>

<font style="color:rgb(77, 77, 77);">首先我们需要知道精确率(Precision)和</font>[<font style="color:rgb(77, 77, 77);">召回率</font>](https://so.csdn.net/so/search?q=%E5%8F%AC%E5%9B%9E%E7%8E%87&spm=1001.2101.3001.7020)<font style="color:rgb(77, 77, 77);">(Recall)，也称为查准率和查全率的定义</font>

**<font style="color:rgb(77, 77, 77);">Precision</font>**<font style="color:rgb(77, 77, 77);">衡量预测有多准确。也就是说，你的所有正向（positive）预测中预测正确的百分比。</font>

**<font style="color:rgb(77, 77, 77);">Recall</font>**<font style="color:rgb(77, 77, 77);">衡量发现所有正例的能力。 例如，你的所有正向（positive）预测中预测正确的占实际正向（positive）个数的比例。</font>

<font style="color:rgb(77, 77, 77);">下面是它们的数学定义：</font>

![](/images/ab45aa0a42fc0130484e9921508bcf1f.jpeg)

<font style="color:rgb(77, 77, 77);">如果对此不太清楚，具体介绍请看</font>[<font style="color:rgb(77, 77, 77);">精确率和召回率的介绍</font>](https://blog.csdn.net/qq_40765537/article/details/105344798)<font style="color:rgb(77, 77, 77);">。</font>

<font style="color:rgb(77, 77, 77);">接下来我们需要知道交并比</font>**<font style="color:rgb(77, 77, 77);">IoU(Intersection over union)</font>**

### 二、IoU <font style="color:rgb(79, 79, 79);">(Intersection over Union)</font>
#### **<font style="color:rgb(79, 79, 79);">1、什么是IoU(Intersection over Union)</font>**
<font style="color:rgb(77, 77, 77);">IoU是一种测量在特定数据集中检测相应物体准确度的一个标准。IoU是一个简单的测量标准，只要是在输出中得出一个预测范围(bounding boxex)的任务都可以用IoU来进行测量。为了可以使IoU用于测量任意大小形状的物体检测，我们需要：</font>

+ <font style="color:rgb(51, 51, 51);">ground-truth bounding boxes（人为在训练集图像中标出要检测物体的大概范围）</font>
+ <font style="color:rgb(51, 51, 51);">我们的算法得出的结果范围。</font>

<font style="color:rgb(77, 77, 77);">也就是说，这个标准用于</font>**<font style="color:rgb(77, 77, 77);">测量真实和预测之间的相关度</font>**<font style="color:rgb(77, 77, 77);">，</font><font style="color:#DF2A3F;">相关度越高，该值越高</font><font style="color:rgb(77, 77, 77);">。如下图所示。绿色标线是人为标记的正确结果（ground-truth），红色标线是算法预测的结果（predicted）。</font>

<font style="color:rgb(77, 77, 77);">  
</font>![](/images/176384d88eca8a9bae79a1922195632a.png)

<font style="color:rgb(77, 77, 77);"></font>

#### **<font style="color:rgb(79, 79, 79);">2、IoU的计算 </font>**
**<font style="color:rgb(77, 77, 77);">IoU是两个区域重叠的部分除以两个区域的集合部分得出的结果，通过设定的阈值，与这个IoU计算结果比较。</font>**

![](/images/80b3dea71df8cd0222e8ce5f1b28a884.png)

<font style="color:rgb(77, 77, 77);"></font>

**<font style="color:rgb(77, 77, 77);">举例如下：</font>**<font style="color:rgb(77, 77, 77);">绿色框是准确值，红色框是预测值。</font>

<font style="color:rgb(77, 77, 77);"></font>![](/images/d77f8ad3c0f74afe45ec3a574653ab59.png)

**<font style="color:rgb(34, 34, 38);">后记</font>**

**<font style="color:rgb(34, 34, 38);">IoU在FCN中称为IU，初看Fully Convolutional Networks for Semantic Segmentation论文，其中的IU概念没有能理解，其实那里的IU也就是IoU，检测物体轮廓不一定非得是方框，也可以是沿着物体的边线：</font>**

![](/images/517cafa94966e95ddb37704a971384cd.jpeg)

**<font style="color:rgb(34, 34, 38);">在实际的任务中，根据不同的任务要求来写不同具体实现的检测方法，但说白了其实都是IoU或者IU。</font>**

**<font style="color:rgb(34, 34, 38);">另外mean IU指的是不同类别识别准确度的平均值，比如一幅图中要识别三个物体，mean IU就是三个物体分别准确度加起来的平均值。</font>**

**<font style="color:rgb(34, 34, 38);">参考资料：</font>**

**<font style="color:rgb(34, 34, 38);">1、</font>**[**https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/**](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)

**<font style="color:rgb(34, 34, 38);">2、</font>**[**https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation/42874377#42874377**](https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation/42874377#42874377)

**<font style="color:rgb(34, 34, 38);">3、</font>**[**https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation/42874377#42874377**](https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation/42874377#42874377)

**<font style="color:rgb(34, 34, 38);">4、</font>**[**https://blog.csdn.net/gaoyu1253401563/article/details/86484851**](https://blog.csdn.net/gaoyu1253401563/article/details/86484851)

**<font style="color:rgb(77, 77, 77);"></font>**

### <font style="color:rgb(79, 79, 79);">三、AP(Average Precision)</font>
参考笔记：[目标检测算法的评估指标：mAP定义及计算方式_map评价指标-CSDN博客](https://blog.csdn.net/weixin_45683778/article/details/131831724)

## 线性层  全连接层  神经元   隐层维度   
线性层(全连接层)公式:

![](/images/2a94f976d7a5211e24038afaf63477ec.png)

+ input：输入数据（形状 [batch_size, input_dim]）。
+ weight：权重矩阵（形状 [output_dim, input_dim]）。
+ bias：偏置向量（形状 [output_dim]）。

作用：将输入数据从 input_dim 维映射到 output_dim 维。

```python
import torch
import torch.nn as nn

# 定义一个线性层：输入100维，输出64维
linear_layer = nn.Linear(100, 64)

# 随机生成输入数据（batch_size=5, 输入维度=100）
input_data = torch.randn(5, 100)

# 前向传播
output = linear_layer(input_data)
print("输出形状:", output.shape)  # 输出: torch.Size([5, 64])

```

关键点

+ **权重矩阵的形状**：`[output_dim, input_dim]`。
+ **偏置的作用**：为每个输出维度提供一个可学习的偏移量。



输入层:输入层一般不算作一个“真正的层”（比如全连接层、卷积层等），它不执行任何操作，只是描述输入数据的 shape（比如 784 维）。



隐层：输入层和输出层之间的层，它们是“中间层”，用于特征提取与变换。



神经元:实际上就是参数矩阵的一行,考虑一个全连接神经网络,输入数据是28*28=784维度,那么经过一个128神经元的全连接层指的就是 参数矩阵是一个大小为 784*128大小的矩阵,  输入数据和参数矩阵转置内积==  1*784 *  784*128 ,每一行内积得到的单个值就是一个神经元运算之后的输出  



假设

```python
输入层（比如 784 维的输入数据，比如展平的 28x28 图像）
       ↓
全连接层1（比如 128 个神经元） → ✅ 这是一个隐层
       ↓
全连接层2（比如 64 个神经元）  → ✅ 这是第二个隐层
       ↓
全连接层3（比如 10 个神经元） → ✅ 通常是输出层（比如做10分类）
```

## 模型
#### 1. O-RCNN框架
O-RCNN（Oriented R-CNN）是一种用于检测旋转目标的改进型区域卷积神经网络框架。它的结构和传统的Faster R-CNN类似，但其候选框（ROI）是旋转的，适合于处理旋转目标检测任务。

#### 2. ViT-B、ViT-L
ViT（Vision Transformer）是基于Transformer架构的图像分类模型。ViT-B和ViT-L分别表示Vision Transformer的基础版本（Base）和大版本（Large）。

+ **ViT-B**（Base）：较小的模型，通常具有12个Transformer层。
+ **ViT-L**（Large）：较大的模型，通常具有24个Transformer层。

#### 3. MAE预训练
MAE（Masked Autoencoders）是一种自监督学习方法，通过将输入图像的一部分遮蔽（masking），然后训练模型重建遮蔽部分，从而学习图像的表示。MAE预训练在大规模无标签数据上进行，之后将预训练模型迁移到具体任务上，如目标检测。

#### 4. HiViT-B
HiViT-B是一种基于Swin-Transformer的变体模型。HiViT-B在Swin-Transformer的基础上，将前两个stage用MLP（多层感知机）代替，从而改进模型性能。

+ **Swin-Transformer**：一种将Transformer应用于计算机视觉任务的新型架构，具有层级特征表示，适合于目标检测和分割任务。
+ **MLP**：多层感知机，是一种基本的神经网络结构。

### Faster R-CNN
Faster R-CNN是目标检测领域的经典模型之一，由Ren等人在2015年提出。它的基本结构分为以下几个部分：

1. **卷积神经网络（CNN）**：用于提取图像特征，常用的骨干网络包括VGG、ResNet等。
2. **区域建议网络（RPN）**：在提取的特征图上生成候选区域（Region Proposals）。RPN输出一组边界框（bounding boxes）和相应的分数，表示这些框包含目标的可能性。
3. **RoI池化（Region of Interest Pooling）**：将RPN生成的候选区域映射到特征图上，通过池化操作将每个候选区域转化为固定大小的特征向量。
4. **分类和回归**：使用全连接层将RoI池化后的特征向量进行分类和回归，输出目标类别和边界框位置的精细调整。

Faster R-CNN的创新点在于引入了RPN，使得候选区域生成与目标检测模型紧密结合，提高了检测速度和精度。

### Swin-Transformer
Swin-Transformer是Microsoft Research Asia的研究人员在2021年提出的一种适用于视觉任务的Transformer模型。其主要特点包括：

1. **层级结构**：Swin-Transformer将图像分割成不重叠的子窗口（windows），在每个窗口内进行自注意力计算，形成层级特征表示。这种层级结构类似于卷积神经网络（CNN）的多层特征提取。
2. **滑动窗口（Shifted Windows）机制**：为了捕捉跨窗口的全局信息，Swin-Transformer引入了滑动窗口机制，即在相邻的Transformer层中，窗口的位置会有所偏移（shifted），从而实现窗口之间的信息交互。
3. **适应性好**：Swin-Transformer可以适应不同分辨率的输入图像，适用于多种计算机视觉任务，包括图像分类、目标检测和语义分割。

Swin-Transformer在多个计算机视觉任务上的表现超过了传统的卷积神经网络（CNN），并且由于其层级结构和滑动窗口机制，在处理大尺寸图像时具有较高的效率和精度。

## 骨干网络（Backbone Network），检测头，neck,FPN特征金字塔
优质文章：[目标检测两个基础部分——backbone and detection head - 玻璃公主 - 博客园](https://www.cnblogs.com/boligongzhu/p/15076220.html)

目标检测和图像分类这两个任务具有一定地相似性，因为可以将分类的网络，比如VGG, ResNet等，用来做特征提取器。这一部分，我们就称其为**backbone骨干网络**。也许它们对图像中目标的位置不够敏感，但至少它们能够提取图片中目标的类别特征。

OD的目标是实现对物体的定位和分类，而图像分类仅仅是对图像中的物体进行分类，而不会去定位。于时，就在“定位”这一点上，是将ImageNet上训练的参数直接加载进来，作为初始参数再去通过这种局部最优点在后续OD任务上

分类网络参数权重迁移过来，用作特征提取器，后续的网络负责从这些特征中，检测目标的位置和类别。当然，两部分会合到一起在OD数据集上去训练，使得它提取出来的特征更适合OD任务。后续连接的网络层由于主要是服务于detection任务，因此称之为“Detection head 检测头”。

FPN特征金字塔结构，在不同的尺度（实际上就是不同大小的feature map）上去提取不同尺度的信息，并进行融合，充分利用好backbone提取的所有的特征信息，从而让网络能够更好的检测物体。有了FPN， backbone提取出的信息可以被利用的更加充分，使得detector能够很好的应对多尺度情况——图像中.

除了FPN这种新颖的结构，还有诸如ASFF、RFB、SPP等好用的模块，都可以插在backbone和detection head之间。由于其插入的位置的微妙，故而将其称之为“neck”，neck这部分的作用就是更好地融合/提取backbone所给出的feature，然后再交由后续的head去检测，从而提高网络的性能。因此，现在一个完整的目标检测网络主要由三部分构成：detector=backbone+neck+head

#### 作用
1. **特征提取**：骨干网络负责提取图像的底层和高层特征，包括边缘、纹理、形状和对象部件等。
2. **特征表示**：通过多层卷积和池化操作，骨干网络将原始图像数据转换为高维特征向量，这些向量能够更好地表示图像中的内容。
3. **数据降维**：通过特征提取过程，骨干网络可以将高维的图像数据降维成易于处理的特征图，减少计算复杂度。

#### 常见的骨干网络
+ **ResNet（Residual Network）**：使用残差连接解决了深度神经网络训练中的梯度消失问题，常用的版本包括ResNet-50、ResNet-101等。
+ **VGGNet**：使用简单的卷积层和池化层堆叠而成，网络深度较深，常用的版本包括VGG-16、VGG-19。
+ **MobileNet**：针对移动设备和嵌入式设备设计的轻量级网络，使用深度可分离卷积减少计算量。
+ **DenseNet（Densely Connected Convolutional Networks）**：通过密集连接（Dense Connection）机制，解决了梯度消失问题，提高了特征复用性。
+ **Swin-Transformer**：基于Transformer架构，通过分割窗口和滑动窗口机制提取图像特征，适用于大尺寸图像。

#### 示例：Faster R-CNN中的骨干网络
在Faster R-CNN框架中，骨干网络位于最前端，用于提取输入图像的特征。以下是具体步骤：

1. **输入图像**：原始输入图像被送入骨干网络。
2. **特征提取**：骨干网络通过多层卷积和池化操作提取特征，输出特征图（Feature Map）。
3. **区域建议网络（RPN）**：特征图被传递给RPN，生成候选区域（Region Proposals）。
4. **ROI池化和分类回归**：候选区域通过ROI池化操作后，进行分类和边界框回归，输出目标类别和位置。

例如，使用ResNet-50作为骨干网络的Faster R-CNN：

+ **ResNet-50**：提取特征并生成特征图。
+ **RPN**：基于特征图生成候选区域。
+ **分类和回归头**：对候选区域进行进一步处理，输出检测结果。

#### 总结
骨干网络是深度学习模型的基础部分，负责将输入图像转换为高维特征表示。这些特征随后被用于更高级的任务，如目标检测、图像分类和语义分割。选择合适的骨干网络可以显著提高模型的性能和效率。



## 预训练模型权重是什么意思
预训练模型权重指的是在一个大型数据集（例如ImageNet）上预先训练好的神经网络模型的参数。这些权重文件包含了模型在预训练过程中学到的特征表示，可以用来初始化你的模型，从而加快训练过程，提高模型的性能。

具体来说，预训练模型权重有以下几个优点：

1. **加快训练速度**：由于模型已经在一个大型数据集上学习到了一些通用的特征，你在自己的数据集上进行训练时，可以从这些预训练的权重开始，而不是从随机初始化的权重开始。这大大减少了训练时间。
2. **提高性能**：预训练的模型已经在大量数据上学到了丰富的特征表示，因此在小数据集上进行微调时，可以更容易地达到更高的性能。
3. **更好的泛化能力**：预训练模型在一个大型、多样的数据集上训练，可以学习到更通用的特征，这有助于模型在新任务上的泛化。

在你的例子中，提供的权重文件（如`mae_vit_small_800e.pth`、`mae_pretrain_vit_base_full.pth`等）是基于ImageNet数据集通过MAE（Masked Autoencoders）方法预训练的。你可以下载这些权重文件并在你的模型中加载，以利用这些预训练的优势。

## 迁移学习 Transfer Learning
迁移学习把任务A开发的模型作为初始点，重新使用在为任务B开发模型的过程中。迁移学习是通过从已学习的相关任务中转移知识来改进学习的新任务。

### 迁移学习的概述
我们主要把迁移学习分为四大类。  
在迁移学习中，有一些Target data，就是和你的任务有直接关系的数据；  
还有很多source data,是和你现在的任务没有直接关系的数据。

根据它们是否有标签，可以分成四类。  
![](/images/ac85c9c878f6942c1cb0f05fb6dd625f.png)

#### 第一类迁移学习 <font style="color:#000000;background-color:rgba(255, 255, 255, 0);"> target data和source data都有标签</font>
<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">我们能做的是1)模型微调  2) 多任务学习 3)渐进式神经网络</font>

##### <font style="color:#000000;background-color:rgba(255, 255, 255, 0);">模型微调</font>
<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">一个比较常见的方法叫保守训练(conservative training)。由于篇幅有限这里不将展开，具体详见李宏毅机器学习课件。</font>

##### <font style="color:#000000;background-color:rgba(255, 255, 255, 0);">多任务学习</font>
<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">背景:有多个不同的任务，我们希望机器能同时学会做好这几个不同的任务</font>

<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">如果这两个任务的输入特征都不能共用</font>

<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">在这两个NN中对不同的输入特征做一些转换，然后丢到共用的网络层中去，再从共用的层中分两个分支出来。</font>

![](/images/1ae66585329c6381f9abf708f39651f5.png)

<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">如果可以选择适当的不同的任务合在一起的话，是可以有帮助的</font>

<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">什么样的任务可能有帮助呢?</font>

![](/images/8825752661976948c626c8317f6e71c2.png)

<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">做语音识别的时候，我们不仅让机器学会某国语言的语音识别，我们让机器学会多国语言的。(人类的语言都会有一些同样的特征，比如中文里面的嘿和英语里面的hey发音很像)从这些共用的层出来后分成多个分支，分别做不同国家语言的语音识别。这整个NN可以同时一起训练，这时候学出来的效果比只用一种语言还要好。</font>

##### <font style="color:#000000;background-color:rgba(255, 255, 255, 0);">3)渐进式网络(Progressive Neural Networks)</font>
![](/images/488090aa0b15472cc2c2d2059c469a47.png)

**<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">Step 1：</font>**<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">构造一个多层的神经网络，训练某一个任务，上图第一列</font>

**<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">Step 2：</font>**<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">构建第二个多层的神经网络，然后固定第一列也就是上一个任务的神经网络，将上一列的神经网络的每一层（注意是每一层）都通过a处理连接到第二列的神经网络的每一层作为额外输入。也就是第二个神经网络每一层除了原始的输入，还加上经过a处理的之前的神经网络对应层的输入。</font>

**<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">Step 3：</font>**<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">构建第三个多层神经网络，训练第三个任务，将前两列的神经网络固定，然后同上一样的方法连接到第三个神经网络中。</font>

<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">上图的线很清楚的表示了这个过程。</font>

<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">这就是把神经网络和神经网络连起来的方法！</font>

**<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">a的作用其实主要是为了降维和输入的维度统一（与原始输入匹配），用简单的MLP来表示！</font>**

<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">除此之外，增强学习算法没有任何变化。文章中使用A3C算法，一个比DQN强4倍的算法！</font>

<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">总的来说，就是</font>**<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">抽取之前的神经网络的信息与当前的输入信息融合，然后训练！训练的效果就可以和没有加前面的神经网络的方法对比，如果效果好很多说明前面的神经网络有用，知识有迁移！</font>**

<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">这种方法的好处就是</font>**<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">之前的训练都保留，不至于像fine tune那样更改原来的网络！而且每一层的特征信息都能得到迁移，并且能够更好的具化分析。</font>**

<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">缺点就是</font>**<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">参数的数量会随着任务的增加而大量增加！并且对于如何连接不同任务的神经网络层以及如何处理这些层的输入和输出，需要依赖人类的专业知识和经验。</font>**

#### 第二类迁<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">移学习 source data有标签，target data无标签</font>
<font style="color:#000000;background-color:rgba(255, 255, 255, 0);">第一种是领域对抗性训练(Domain Adversarial Training)，第二种是零次学习(Zero-shot Learning)。</font>

##### <font style="color:#000000;background-color:rgba(255, 255, 255, 0);">1)领域对抗性训练(Domain Adversarial Training)</font>
这种情况的前提是他们有相同的任务，在概念上你可以把有标签的source data当成训练数据，把无标签的target data当成测试数据，但是这样的效果肯定是很差的，因为它们的分布不同。

假设今天要做手写数字识别，你有有标签的MNIST的数据，但是你要识别的对象是无标签的来自MNIST-M的数据，在MNIST-M中的数字甚至是彩色的，它的数据样本分布和原来的MNIST分布不一样。所以需要特别的处理。

![](/images/eec76bfbac4d7695eb075e0b558518e9.png)

Domain-adversarial training就是干这件事的。Domain-adversarial training**可以看成GAN的一种**。它想要把source data和target data转换到同样的领域上，让它们有同样的分布。

![](/images/c4a41b038b23093131378a823f822a07.png)



如果我们没有对数据做任何处理，单纯的拿source data来训练一个分类器，它输入是一个图像，输出是该图形的类别。那今天得到的特征分布可能是下面这样子。

![](/images/aa817575c187826e9f451c657d64db65.png)

MNIST的数据它是蓝色的点，确实可以看到它们分成一群一群的，把几群数据的点拿出来看的话，得到的结果可能是左边的样子，能区分出4,0和1。 但是把和MNIST分布不同的MNIST-M手写数字的图片丢到这个分类器中去，这些不一样的图片，它们的特征分布可能像红点一样。可以看到，红点和蓝点根本没有交集。

如果今天这个NN无法用同样的特征表示这两种数据，那么就会无法得到好的分类结果。

我们希望在一个NN中，前面几个网络层做的事是特征抽取，如图1所示,也就是说，希望这个特征抽取器能把不同领域的source data和target data都转成同样的特征。

![](/images/ddfc768644348c2c4eab0f2151d2a555.png)

图1 Feature Extractor：特征提取器<font style="color:#000000;"></font>

也就是我们希望说，红点和蓝点的分布不是上面这样，而是像下面混合在一起。

![](/images/2ea1de54012709172aa28696d0f295b9.png)

那怎么让我们这个特征抽取器做到这件事情呢。

这里需要引入一个**领域的分类器(domain classifier)**，如图2所示，**就像我们做GAN的时候引入的鉴别器。它也是一个神经网络。**

![](/images/9be1bcfe6c15aee16527ff0fbb89d579.png)

图2 Domain Classifier领域的分类器

Domain-adversarial training可以看成GAN的一种。它想要把source data和target data转换到同样的领域上，让它们有同样的分布。

这个领域分类器的作用是，要侦测出现在特征抽取器输出的特征是属于哪个领域的(来自哪个分布的)。现在特征抽取器要做的事情是尽量骗过这个领域分类器，而后者是尽量防止被骗。

特征抽取器要做的是去除source 领域和target 领域不一样的地方，让提取出来的特征分布是很接近的，可以骗过领域分类器。但是如果只有这两个神经网络是不够的。因为绿色的特征抽取器可以轻易的骗过红色的分类器，只要它不管输入是什么，只把所有的输出都变成0就可以了。

所以需要引入另外一个东西叫**标签预测器(Label predictor)**的东西。

![](/images/e650c6f029740173ba6d2bf82425c4f4.png)

图3 Label predictor：标签预测器

现在特征抽取器不仅要骗过分类器，还要让预测器尽量有准确的预测结果。这是一个很大的神经网络，但是这三个不同的部分有不同的目标。

预测器想要正确的分类输入的图片，分类器想要正确分别输入是来自哪个分布。它们都只能看到特征抽取器抽取后的特征。

抽取器一方面希望可以促使预测器做的好，另一方面要防止分类器做的好。

那么**<font style="color:#DF2A3F;">要怎么做</font>**呢？

一样用梯度下降来训练，红色的分类器部分要调整参数，去让分辨领域的结果越正确越好；蓝色的预测器需要调参数，让标签的预测正确率越高越好；如图4所示梯度反向传播过程。

![](/images/47106f4cd2b865c32d55aba2086da832.png)

这两者不一样的地方在于，当分类器要求绿色的抽取器去调整参数以满足以及的目标时，绿色的抽取器会尽量满足它的要求；还当红色的神经网络要求绿色的神经网络调整参数的时候，红色的网络会故意乘以− 1，以防止分类器做的好。

最后红色的神经网路会无法做好分类，但是它必须要努力挣扎，它需要从绿色的NN给的不好的特征里面尽量去区分它们的领域。这样才能迫使绿色的NN产生红色的NN无法分辨的特征。难点就在于让红色的NN努力挣扎而不是很快放弃。

##### 2)零次学习（Zero-shot Learning）
零次学习(Zero-shot Learning)说的是source data和target data它们的任务都不相同。

比如source data可能是要做猫和狗的分类；但是target data要做的是做草泥马和羊的分类。

![](/images/d05871a5f6b4ea71f685bce95723c526.png)

target data中需要正确找出草泥马，但是source data中都没出现过草泥马，那要怎么做这件事情呢

我们先看下语音识别里面是怎么做的，语音识别一直都有训练数据(source data)和测试数据(target data)是不同任务的问题。 很有可能在测试数据中出现的词汇，在训练数据中从来没有出现过。语音识别在处理这个问题的时候，做法是找出比词汇更小的单位。通常语音识别都是拿音位(phoneme，可以理解为音标)做为单位。

如果把词汇都转成音位，在识别的时候只去识别音位，然后再把音位转换为词汇的话就可以解决训练数据和测试数据不一样的问题。

在图像上的处理方法也很类似，这里不展开。

#### 第三类迁移学习
##### 自我学习
自我学习(Self-taught learning)其实和半监督学习很像，都是有少量的有标签数据，和非常多的无标签数据。但是与半监督学习有个很大的不同是，有标签数据可能和无标签数据是没有关系的。

####  第四类迁移学习
##### 自学成簇
如果target data和source data都是无标签的话，可以用Self-taught Clustering来做。

可以用无标签的source data，可以学出一个较好的特征表示，再用这个较好的特征表示用在聚类上，就可以得到较好的结果。
