---
title: 数据集读取及处理
date: '2025-08-05 14:28:04'
updated: '2025-08-22 16:19:43'
---
### **<font style="color:rgb(0, 0, 0);"> build(image_set, args):</font>**
```plain
if args.test:
    ann_file = Path(args.vidstg_ann_path) / f"test.json"
elif image_set == "val":
    ann_file = Path(args.vidstg_ann_path) / f"val.json"
else:
    ann_file = (
        Path(args.vidstg_ann_path) / f"train.json"
        if args.video_max_len_train == 200 or (not args.sted)
        else Path(args.vidstg_ann_path) / f"train_{args.video_max_len_train}.json"
    )
```

+ **<font style="color:rgb(0, 0, 0);">逻辑分支</font>**<font style="color:rgb(0, 0, 0);">：</font>
    - **<font style="color:rgb(0, 0, 0);">测试集</font>**<font style="color:rgb(0, 0, 0);">：固定使用</font><font style="color:rgb(0, 0, 0);"> </font>`<font style="color:rgb(0, 0, 0);">test.json</font>`
    - **<font style="color:rgb(0, 0, 0);">验证集</font>**<font style="color:rgb(0, 0, 0);">：固定使用</font><font style="color:rgb(0, 0, 0);"> </font>`<font style="color:rgb(0, 0, 0);">val.json</font>`
    - **<font style="color:rgb(0, 0, 0);">训练集</font>**<font style="color:rgb(0, 0, 0);">：</font>
        * <font style="color:rgb(0, 0, 0);">默认使用</font><font style="color:rgb(0, 0, 0);"> </font>`<font style="color:rgb(0, 0, 0);">train.json</font>`<font style="color:rgb(0, 0, 0);">（当视频最大长度=200帧 </font>**<font style="color:rgb(0, 0, 0);">或</font>**<font style="color:rgb(0, 0, 0);"> 不使用时空定位</font>`<font style="color:rgb(0, 0, 0);">sted=False</font>`<font style="color:rgb(0, 0, 0);">）</font>
        * <font style="color:rgb(0, 0, 0);">否则使用动态命名的</font><font style="color:rgb(0, 0, 0);"> </font>`<font style="color:rgb(0, 0, 0);">train_{max_len}.json</font>`<font style="color:rgb(0, 0, 0);">（例如</font><font style="color:rgb(0, 0, 0);"> </font>`<font style="color:rgb(0, 0, 0);">train_100.json</font>`<font style="color:rgb(0, 0, 0);">）</font>
+ **<font style="color:rgb(0, 0, 0);">设计意图</font>**<font style="color:rgb(0, 0, 0);">：</font>
    - <font style="color:rgb(0, 0, 0);">支持不同长度的训练集配置（通过</font>`<font style="color:rgb(0, 0, 0);">video_max_len_train</font>`<font style="color:rgb(0, 0, 0);">参数控制）</font>
    - <font style="color:rgb(0, 0, 0);">当不需要时空定位时简化数据加载（</font>`<font style="color:rgb(0, 0, 0);">sted=False</font>`<font style="color:rgb(0, 0, 0);">）</font>

```python
dataset = VideoModulatedSTGrounding(
        vid_dir,		# 视频文件根目录 arg.vidstg_vid_path
        ann_file,		#args.vidstg_ann_path/标注文件
        transforms=make_video_transforms(	#视频数据增强
            image_set, cautious=True, resolution=args.resolution
        ),
        is_train=image_set == "train",
        video_max_len=args.video_max_len,
        video_max_len_train=args.video_max_len_train,
        fps=args.fps,
        tmp_crop=args.tmp_crop and image_set == "train",  #是否启用时序裁剪  默认 true
        tmp_loc=args.sted,		#使用时序定位标注  启用sted损失
        stride=args.stride,		#时序采样步长
    )
```

### **<font style="color:rgb(0, 0, 0);">数据增强策略</font>**
```plain
transforms=make_video_transforms(
    image_set, 
    cautious=True, 
    resolution=args.resolution  #默认resolution 224
)  
```

<font style="color:#000000;">函数有三个参数：</font>

1. `<font style="color:#000000;">image_set</font>`<font style="color:#000000;">：字符串，通常是 'train'、'val' 或 'test'，表示是训练集、验证集还是测试集。</font>
2. `<font style="color:#000000;">cautious</font>`<font style="color:#000000;">：布尔值，如果为True，则在空间随机裁剪时会小心处理，避免破坏边界框标注（比如，不会把标注的物体裁掉）。</font>
3. `<font style="color:#000000;">resolution</font>`<font style="color:#000000;">：整数，表示我们希望视频帧的短边调整到的目标分辨率（默认是224）</font>

返回值是一个组合操作对象

<font style="color:#000000;">函数内部首先定义了一个标准化的操作（</font>`<font style="color:#000000;">normalizeop</font>`<font style="color:#000000;">），它包含两个步骤：</font>

+ `<font style="color:#000000;">ToTensor()</font>`<font style="color:#000000;">：将图像（numpy数组或者PIL图像）转换成PyTorch张量，并且将像素值从0-255归一化到0-1之间。</font>
+ `<font style="color:#000000;">Normalize</font>`<font style="color:#000000;">：用ImageNet的均值和标准差进行标准化（减去均值然后除以标准差）。</font>

<font style="color:#000000;">函数根据输入的</font>`<font style="color:#000000;">resolution</font>`<font style="color:#000000;">参数，选择一系列预定义的参数（比如缩放尺度、最大尺寸、裁剪尺寸等）。这些参数用于后续的随机缩放和裁剪。</font>

<font style="color:#000000;"></font>

<font style="color:#000000;">训练集的变换包括：</font>

+ <font style="color:#000000;">可选的随机水平翻转（如果</font>`<font style="color:#000000;">cautious</font>`<font style="color:#000000;">为False，就加入水平翻转；如果为True，就不加，因为翻转可能会改变空间关系，而有些任务可能对空间关系敏感，比如需要小心保护边界框的任务）。</font>
+ <font style="color:#000000;">然后是一个随机选择（RandomSelect）：</font>
    - <font style="color:#000000;">选项一：直接随机缩放到一个尺寸（在</font>`<font style="color:#000000;">scales</font>`<font style="color:#000000;">列表中随机选一个，同时限制最大边不超过</font>`<font style="color:#000000;">max_size</font>`<font style="color:#000000;">）。</font>
    - <font style="color:#000000;">选项二：一个组合变换，包括：  
</font><font style="color:#000000;">a. 先随机缩放到</font>`<font style="color:#000000;">resizes</font>`<font style="color:#000000;">列表中的一个尺寸。  
</font><font style="color:#000000;">b. 然后进行随机尺寸的裁剪（</font>`<font style="color:#000000;">RandomSizeCrop</font>`<font style="color:#000000;">），裁剪到</font>`<font style="color:#000000;">crop</font>`<font style="color:#000000;">尺寸，并且最大边不超过</font>`<font style="color:#000000;">max_size</font>`<font style="color:#000000;">。这里注意参数</font>`<font style="color:#000000;">respect_boxes</font>`<font style="color:#000000;">，如果</font>`<font style="color:#000000;">cautious</font>`<font style="color:#000000;">为True，那么裁剪会尊重边界框，确保不裁掉有标注的区域。  
</font><font style="color:#000000;">c. 再进行一次随机缩放（和选项一相同的缩放操作）。</font>
+ <font style="color:#000000;">最后是标准化操作。</font>

#### <font style="color:rgb(0, 0, 0);">这样设计？</font>
1. **<font style="color:rgb(0, 0, 0);">模拟不同视角</font>**<font style="color:rgb(0, 0, 0);">：</font>
    - <font style="color:rgb(0, 0, 0);">裁剪相当于改变"摄像机位置"</font>
    - <font style="color:rgb(0, 0, 0);">缩放模拟"变焦"效果</font>
2. **<font style="color:rgb(0, 0, 0);">增加数据多样性</font>**<font style="color:rgb(0, 0, 0);">：</font>
    - <font style="color:rgb(0, 0, 0);">同一视频帧在不同训练周期产生不同变体</font>
    - <font style="color:rgb(0, 0, 0);">防止模型记住特定像素位置</font>

### <font style="color:#000000;">视频时空定位任务的数据集类  VideoModulatedSTGrounding</font>
:::success
<font style="color:rgba(0, 0, 0, 0.6);">主要作用是：</font>

+ **<font style="color:rgba(0, 0, 0, 0.6);">加载视频数据</font>**<font style="color:rgba(0, 0, 0, 0.6);">：从磁盘读取视频文件（通过ffmpeg）</font>
+ **<font style="color:rgba(0, 0, 0, 0.6);">处理标注信息</font>**<font style="color:rgba(0, 0, 0, 0.6);">：解析时空定位的标注（目标物体在哪些帧出现，以及位置）</font>
+ **<font style="color:rgba(0, 0, 0, 0.6);">数据预处理</font>**<font style="color:rgba(0, 0, 0, 0.6);">：包括空间变换（如裁剪、缩放）和时间裁剪（如截取片段）</font>
+ **<font style="color:rgba(0, 0, 0, 0.6);">组织数据格式</font>**<font style="color:rgba(0, 0, 0, 0.6);">：返回模型训练/验证所需的格式</font>

:::

| <font style="color:rgb(0, 0, 0);">处理阶段</font> | `<font style="color:rgb(0, 0, 0);">__init__</font>`<br/><font style="color:rgb(0, 0, 0);">预处理</font> | `<font style="color:rgb(0, 0, 0);">__getitem__</font>`<br/><font style="color:rgb(0, 0, 0);">运行时处理</font> |
| :---: | :---: | :---: |
| **<font style="color:rgb(0, 0, 0);">执行时机</font>** | <font style="color:rgb(0, 0, 0);">数据集加载时一次性执行</font> | <font style="color:rgb(0, 0, 0);">每次获取样本时动态执行</font> |
| **<font style="color:rgb(0, 0, 0);">主要目的</font>** | <font style="color:rgb(0, 0, 0);">建立全局帧索引映射</font> | <font style="color:rgb(0, 0, 0);">应用随机增强/适应模型输入</font> |
| **<font style="color:rgb(0, 0, 0);">操作性质</font>** | <font style="color:rgb(0, 0, 0);">确定性计算</font> | <font style="color:rgb(0, 0, 0);">随机性增强</font> |
| **<font style="color:rgb(0, 0, 0);">影响范围</font>** | <font style="color:rgb(0, 0, 0);">整个数据集</font> | <font style="color:rgb(0, 0, 0);">单个样本</font> |
| **<font style="color:rgb(0, 0, 0);">典型操作</font>** | <font style="color:rgb(0, 0, 0);">基础帧采样、元数据组织</font> | <font style="color:rgb(0, 0, 0);">随机裁剪、长度压缩</font> |


#### __init__初始化操作:
```plain
        self.vid2imgids = (
            {}
        )  # map video_id to [list of frames to be forwarded, list of frames in the annotated moment]
        self.stride = stride
        for i_vid, video in enumerate(self.annotations["videos"]):
            
            #这里计算采样率 =  视频fps / 视频原始fps
            video_fps = video["fps"]  # used for extraction
            sampling_rate = fps / video_fps
            assert sampling_rate <= 1  # 同时只支持降采样downsampling at fps
            
            
            start_frame = (
                video["start_frame"] if self.tmp_loc else video["tube_start_frame"]
            )
            end_frame = video["end_frame"] if self.tmp_loc else video["tube_end_frame"]
            frame_ids = [start_frame]

            #通过比较 当前帧×采样率 的整数部分与 上一采样帧×采样率 的整数部分
            for frame_id in range(start_frame, end_frame):
                if int(frame_ids[-1] * sampling_rate) < int(frame_id * sampling_rate):
                    frame_ids.append(frame_id)

            # 如果视频帧数超过最大长度，则进行下采样(上面是固定采样率采集完一个视频,但是参数video_max_len会限制视频的最大长度,所以进行第二次下采样)
            if len(frame_ids) > video_max_len:  # subsample at video_max_len
                frame_ids = [
                    frame_ids[(j * len(frame_ids)) // video_max_len]
                    for j in range(video_max_len)
                ]

            #再从已选帧中筛选出位于标注动作区间内的帧
            inter_frames = set(
                [
                    frame_id
                    for frame_id in frame_ids
                    if video["tube_start_frame"] <= frame_id < video["tube_end_frame"]
                ]
            )  # frames in the annotated moment
            
            #使用vid2imgids字典记录视频ID对应的帧列表和标注动作帧列表
            self.vid2imgids[video["video_id"]] = [frame_ids, inter_frames]
```

<font style="color:rgba(0, 0, 0, 0.6);">总结：这段代码的作用是，对每个视频，按照指定的帧率（</font>`<font style="color:rgba(0, 0, 0, 0.6);">fps</font>`<font style="color:rgba(0, 0, 0, 0.6);">）抽取帧，并确保抽取的帧数不超过最大长度（</font>`<font style="color:rgba(0, 0, 0, 0.6);">video_max_len</font>`<font style="color:rgba(0, 0, 0, 0.6);">）。同时，记录下这些帧中哪些帧属于标注的时刻（即我们要定位的片段）。</font>

**<font style="color:rgb(0, 0, 0);">关键特点</font>**<font style="color:rgb(0, 0, 0);">：</font>

+ <font style="color:rgb(0, 0, 0);">纯数学计算，不涉及实际视频解码</font>
+ <font style="color:rgb(0, 0, 0);">生成的是</font>**<font style="color:rgb(0, 0, 0);">帧索引列表</font>**<font style="color:rgb(0, 0, 0);">而非真实帧数据</font>
+ <font style="color:rgb(0, 0, 0);">确保所有视频有统一的基准采样率</font>
+ <font style="color:rgb(0, 0, 0);">结果存储在</font>`<font style="color:rgb(0, 0, 0);">self.vid2imgids</font>`<font style="color:rgb(0, 0, 0);">供后续使用</font>

**<font style="color:rgb(0, 0, 0);">为什么需要如此复杂的采样？</font>**

<font style="color:rgb(0, 0, 0);">1. 解决视频时长差异</font>

<font style="color:rgb(0, 0, 0);">不同视频时长不同，需要统一处理成固定长度：</font>

+ <font style="color:rgb(0, 0, 0);">长视频：降采样</font>
+ <font style="color:rgb(0, 0, 0);">短视频：保持原样</font>

<font style="color:rgb(0, 0, 0);">2.时间一致性</font>

<font style="color:rgb(0, 0, 0);">采样算法保证：</font>

+ <font style="color:rgb(0, 0, 0);">固定时间间隔采样（时间均匀）</font>
+ <font style="color:rgb(0, 0, 0);">动作帧必定被包含（关键帧保护）</font>

####  __getitem__(self, idx)操作
<font style="color:rgba(0, 0, 0, 0.6);">这个方法负责从数据集中获取一个视频样本，处理视频帧，准备标注，并应用必要的变换。</font>

```python

    def __getitem__(self, idx):
        """
        :param idx: int
        :return:
        images: a CTHW video tensor
        targets: list of frame-level target, one per frame, dictionary with keys image_id, boxes, orig_sizes
        tmp_target: video-level target, dictionary with keys video_id, qtype, inter_idx, frames_id, caption
        """
        # 1. 获取视频元数据
        video = self.annotations["videos"][idx]  # 获取索引为idx的视频元数据字典
        caption = video["caption"]  # 视频描述文本（如"女孩拿起茶杯"）
        video_id = video["video_id"]  # 视频唯一标识符
        video_original_id = video["original_video_id"]  # 原始视频ID（可能来自源数据集）
        
        # 2. 确定视频片段范围
        clip_start = video["start_frame"]  # 视频片段起始帧（包含）
        clip_end = video["end_frame"]  # 视频片段结束帧（不包含）
        
        # 3. 获取预计算的帧信息
        frame_ids, inter_frames = self.vid2imgids[video_id]
        # 4. 获取目标轨迹数据
        trajectory = self.annotations["trajectories"][video_original_id][
            str(video["target_id"])
        ]# trajectory: 目标物体在视频中的运动轨迹字典 {帧ID: 边界框坐标}


        # ffmpeg decoding   视频解码处理
        # 5. 构建视频文件路径
        vid_path = os.path.join(self.vid_folder, "video", video["video_path"])
        
        # 6. 获取视频参数
        video_fps = video["fps"]  # 原始视频帧率
        ss = clip_start / video_fps  # 起始时间（秒）
        t = (clip_end - clip_start) / video_fps  # 片段时长（秒）
        
        # 7. 使用ffmpeg精确提取所需帧
        cmd = ffmpeg.input(vid_path, ss=ss, t=t).filter("fps", fps=len(frame_ids)/t)
        # 关键点：fps=len(frame_ids)/t 确保精确提取所需数量的帧
        
        # 8. 运行ffmpeg获取原始视频数据
        out, _ = cmd.output("pipe:", format="rawvideo", pix_fmt="rgb24").run(
            capture_stdout=True, quiet=True
        )
        
        # 9. 转换原始字节为numpy数组
        w = video["width"]  # 视频宽度
        h = video["height"]  # 视频高度
        # 将字节流转换为uint8数组并重塑为视频帧形状
        images_list = np.frombuffer(out, np.uint8).reshape([-1, h, w, 3])
        # 形状：[帧数, 高度, 宽度, 通道] (NHWC格式)
        
        # 10. 验证帧数匹配
        assert len(images_list) == len(frame_ids)  # 确保提取的帧数与计划一致

        # prepare frame-level targets  目标标注处理  
        targets_list = []
        inter_idx = []  # list of indexes of the frames in the annotated moment
        for i_img, img_id in enumerate(frame_ids):
            if img_id in inter_frames:
                anns = trajectory[
                    str(img_id)
                ]  # dictionary with bbox [left, top, width, height] key
                anns = [anns]
                inter_idx.append(i_img)
                anns = []
            target = prepare(w, h, anns)
            target["image_id"] = f"{video_id}_{img_id}"
            targets_list.append(target)

        # video spatial transform
        if self._transforms is not None:
            images, targets = self._transforms(images_list, targets_list)
            # 返回:
            # images: 形状[C, T, H, W]的张量 (CTHW格式)
            # targets: 变换后的标注列表
            
        else:
            images, targets = images_list, targets_list

        if (
            inter_idx   #验证动作帧标注完整性
        ):  # number of boxes should be the number of frames in annotated moment
            assert (
                len([x for x in targets if len(x["boxes"])])
                == inter_idx[-1] - inter_idx[0] + 1
            ), (len([x for x in targets if len(x["boxes"])]), inter_idx)

        # temporal crop 时序处理（关键步骤）
        if self.tmp_crop:
            p = random.random()
            if p > 0.5:  # random crop
                # list possible start indexes
                 # 确定裁剪起始点
                if inter_idx:
                    # 18. 动作帧存在时，只考虑动作开始前的起始点
                    starts_list = [i for i in range(len(frame_ids)) if i < inter_idx[0]]
                else:
                    starts_list = [i for i in range(len(frame_ids))]

                # sample a new start index  选择新起始点
                if starts_list:
                    new_start_idx = random.choice(starts_list)
                else:
                    new_start_idx = 0

                # list possible end indexes  确定裁剪结束点
                if inter_idx:
                     # 只考虑动作结束后的结束点
                    ends_list = [i for i in range(len(frame_ids)) if i > inter_idx[-1]]
                else:
                    ends_list = [i for i in range(len(frame_ids)) if i > new_start_idx]

                # sample a new end index 选择新结束点
                if ends_list:
                    new_end_idx = random.choice(ends_list)
                else:
                    new_end_idx = len(frame_ids) - 1

                # update everything  更新数据
                prev_start_frame = frame_ids[0]
                prev_end_frame = frame_ids[-1]
                frame_ids = [
                    x
                    for i, x in enumerate(frame_ids)
                    if new_start_idx <= i <= new_end_idx
                ]
                ## 24. 裁剪视频张量 (CTHW格式)
                images = images[:, new_start_idx : new_end_idx + 1]  # CTHW
                
                targets = [ ## 25. 裁剪标注
                    x
                    for i, x in enumerate(targets)
                    if new_start_idx <= i <= new_end_idx
                ]
                clip_start += frame_ids[0] - prev_start_frame
                clip_end += frame_ids[-1] - prev_end_frame
                # 27. 更新动作帧索引
                if inter_idx:
                    inter_idx = [x - new_start_idx for x in inter_idx]

        #训练时帧采样处理
        if ( #训练时帧数压缩
            self.is_train and len(frame_ids) > self.video_max_len_train
        ):  # densely sample video_max_len_train frames
            if inter_idx:
                starts_list = [
                    i
                    for i in range(len(frame_ids))
                    if inter_idx[0] - self.video_max_len_train < i <= inter_idx[-1]
                ]
            else:
                starts_list = [i for i in range(len(frame_ids))]

            # sample a new start index
            if starts_list:
                new_start_idx = random.choice(starts_list)
            else:
                new_start_idx = 0

            # select the end index
            new_end_idx = min(
                new_start_idx + self.video_max_len_train - 1, len(frame_ids) - 1
            )

            # update everything
            prev_start_frame = frame_ids[0]
            prev_end_frame = frame_ids[-1]
            frame_ids = [
                x for i, x in enumerate(frame_ids) if new_start_idx <= i <= new_end_idx
            ]
            images = images[:, new_start_idx : new_end_idx + 1]  # CTHW
            targets = [
                x for i, x in enumerate(targets) if new_start_idx <= i <= new_end_idx
            ]
            clip_start += frame_ids[0] - prev_start_frame
            clip_end += frame_ids[-1] - prev_end_frame
            if inter_idx:
                inter_idx = [
                    x - new_start_idx
                    for x in inter_idx
                    if new_start_idx <= x <= new_end_idx
                ]

        # video level annotations
        # 34. 构建视频级目标
        tmp_target = {
            "video_id": video_id,
            "qtype": video["qtype"],  # 问题类型
            # 动作帧索引范围（如[5, 25]表示第5帧到第25帧）
            "inter_idx": [inter_idx[0], inter_idx[-1]] if inter_idx else [-100, -100],
            "frames_id": frame_ids,  # 使用的帧ID列表
            "caption": caption  # 视频描述文本
        }
    
        if self.stride:
            return images[:, :: self.stride], targets, tmp_target, images
        return images, targets, tmp_target
```

返回：

    images: 形状为[C,T,H,W]的视频张量 (C=通道数, T=帧数, H=高度, W=宽度)

    targets: 每帧的目标检测信息列表

    tmp_target: 视频级别的目标信息字典

**<font style="color:rgb(0, 0, 0);">关键特点</font>**<font style="color:rgb(0, 0, 0);">：</font>

+ <font style="color:rgb(0, 0, 0);">基于</font>`<font style="color:rgb(0, 0, 0);">__init__</font>`<font style="color:rgb(0, 0, 0);">生成的索引进行实际操作</font>
+ <font style="color:rgb(0, 0, 0);">包含随机性（每次调用可能结果不同）</font>
+ <font style="color:rgb(0, 0, 0);">直接影响输入模型的张量形状</font>
+ <font style="color:rgb(0, 0, 0);">处理的是已经解码的真实视频数据</font>

关键代码

```python
def getitem(self, idx):
    # 1. 获取预计算的帧索引
    frame_ids, inter_frames = self.vid2imgids[video_id]  # 来自__init__的结果
    # 2. 实际解码视频帧（此时才接触真实像素数据）
    images = decode_frames(frame_ids)  # [T,H,W,C]

    # 3. 时序裁剪（50%概率随机裁剪）
    if self.tmp_crop and random.random() > 0.5:
        # 在保持动作帧完整的前提下随机选取区间
        new_start, new_end = random_crop_interval(frame_ids, inter_frames)
        frame_ids = frame_ids[new_start:new_end]
        images = images[new_start:new_end]

    # 4. 训练时帧压缩（如从100帧→50帧）
    if self.is_train and len(frame_ids) > self.video_max_len_train:
        # 随机选取连续50帧（优先保留动作帧）
        start = random_select_start(frame_ids, inter_frames, target_len=50)
        frame_ids = frame_ids[start:start+50]
        images = images[start:start+50]
```

![](/images/334f917c8426691d8e5fe958cd392f71.png)                               ![](/images/b50ea84e2b35dced896342cf1e502c7a.png)

| <font style="color:rgb(0, 0, 0);">特性</font> | <font style="color:rgb(0, 0, 0);">时序裁剪 (Temporal Crop)</font> | <font style="color:rgb(0, 0, 0);">训练时帧压缩 (Train-time Frame Compression)</font> |
| :---: | :---: | :---: |
| **<font style="color:rgb(0, 0, 0);">目的</font>** | <font style="color:rgb(0, 0, 0);">增加数据多样性，模拟不同观察时长</font> | <font style="color:rgb(0, 0, 0);">适配GPU显存，统一输入尺寸</font> |
| **<font style="color:rgb(0, 0, 0);">触发条件</font>** | `<font style="color:rgb(0, 0, 0);">tmp_crop=True</font>`<br/><font style="color:rgb(0, 0, 0);"> </font><font style="color:rgb(0, 0, 0);">+ 随机概率>0.5</font> | `<font style="color:rgb(0, 0, 0);">is_train=True</font>`<br/><font style="color:rgb(0, 0, 0);"> </font><font style="color:rgb(0, 0, 0);">+ 帧数></font>`<font style="color:rgb(0, 0, 0);">video_max_len_train</font>` |
| **<font style="color:rgb(0, 0, 0);">随机性</font>** | <font style="color:rgb(0, 0, 0);">完全随机选择区间</font> | <font style="color:rgb(0, 0, 0);">在动作帧附近随机选择</font> |
| **<font style="color:rgb(0, 0, 0);">长度变化</font>** | <font style="color:rgb(0, 0, 0);">可变长度输出</font> | <font style="color:rgb(0, 0, 0);">固定压缩到</font>`<font style="color:rgb(0, 0, 0);">video_max_len_train</font>` |
| **<font style="color:rgb(0, 0, 0);">动作帧处理</font>** | <font style="color:rgb(0, 0, 0);">必须完整包含所有动作帧</font> | <font style="color:rgb(0, 0, 0);">可能只包含部分动作帧</font> |
| **<font style="color:rgb(0, 0, 0);">典型值</font>** | <font style="color:rgb(0, 0, 0);">裁剪后长度不定</font> | <font style="color:rgb(0, 0, 0);">通常压缩到100-200帧</font> |


**<font style="color:rgb(0, 0, 0);">为什么需要两个操作？</font>**

**<font style="color:rgb(0, 0, 0);">时序裁剪解决的核心问题：</font>**

1. <font style="color:rgb(0, 0, 0);">模拟不同观察时长（短视频片段vs完整视频）</font>
2. <font style="color:rgb(0, 0, 0);">增强模型对部分观察的鲁棒性</font>
3. <font style="color:rgb(0, 0, 0);">数据增强防止过拟合</font>

**<font style="color:rgb(0, 0, 0);">帧压缩解决的核心问题：</font>**

1. <font style="color:rgb(0, 0, 0);">统一输入尺寸适应GPU并行处理</font>
2. <font style="color:rgb(0, 0, 0);">控制显存消耗（长视频→短片段）</font>
3. <font style="color:rgb(0, 0, 0);">训练稳定性（避免batch内长度差异过大）</font>

**<font style="color:rgb(0, 0, 0);">这种组合实现了：</font>**

1. **<font style="color:rgb(0, 0, 0);">多尺度训练</font>**<font style="color:rgb(0, 0, 0);">：通过时序裁剪看到不同长度片段</font>
2. **<font style="color:rgb(0, 0, 0);">高效训练</font>**<font style="color:rgb(0, 0, 0);">：通过帧压缩保证batch内统一长度</font>
3. **<font style="color:rgb(0, 0, 0);">完整学习</font>**<font style="color:rgb(0, 0, 0);">：确保关键动作不被截断</font>

