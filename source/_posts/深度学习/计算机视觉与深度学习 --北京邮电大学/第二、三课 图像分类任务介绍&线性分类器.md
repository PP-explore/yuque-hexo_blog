---
title: 第二、三课 图像分类任务介绍&线性分类器
date: '2024-07-02 14:28:20'
updated: '2025-08-22 18:00:34'
categories:
  - 人工智能
tags:
  - 深度学习
cover: /images/custom-cover.jpg
recommend: true
---
# <font style="color:#000000;">一、本章解决什么问题</font>
<font style="color:#000000;">1.什么是图像分类任务？它有哪些应用场合？  
</font><font style="color:#000000;">(答：找特征，贴标签）  
</font><font style="color:#000000;">2.图像分类任务有哪些难点？  
</font><font style="color:#000000;">3.基于规则的方法是否可行？  
</font><font style="color:#000000;">4.什么是数据驱动的图像分类范式？  
</font><font style="color:#000000;">5.常用的分类任务评价指标是什么？</font>

# <font style="color:#000000;">二、图像分类任务有哪些难点</font>
+ <font style="color:#000000;">1.从广义来讲：跨越语义鸿沟</font>
+ <font style="color:#000000;">2.视角：比如同一人脸的不同角度</font>
+ <font style="color:#000000;">3.光照：背光和面光造成色彩变化极大</font>
+ <font style="color:#000000;">4.尺度：车载识别系统中，远近大小的人都要识别到</font>
+ <font style="color:#000000;">5.遮挡：虹膜，只露出一半的虹膜</font>
+ <font style="color:#000000;">6.形变：比如猫是液体的。。。。。</font>
+ <font style="color:#000000;">7.背景杂波：雪地中的北极狐，背景对图像造成干扰。</font>
+ <font style="color:#000000;">8.类内形变：各种设计感强的凳子</font>
+ <font style="color:#000000;">9.运动模糊：感应器成像的技术性特点，对我们图像识别造成的困扰。</font>
+ <font style="color:#000000;">10.类别繁多：</font>



# <font style="color:#000000;">什么是数据驱动的图像分类范式（28：28）</font>
机器学习的方法解决图像处理

## <font style="color:#000000;">（一）数据驱动的图像分类方法的</font><font style="color:#DF2A3F;">步骤</font>
+ <font style="color:#000000;">1. 数据集构建</font>
    - <font style="color:#000000;">1.1 有监督数据集</font>
    - <font style="color:#000000;">1.2 无监督的数据集</font>
+ <font style="color:#000000;">2.分类器设计与学习（核心）</font>
    - <font style="color:#000000;">2.1 数学模型</font>
    - <font style="color:#000000;">2.2 根据数学模型构建分类器</font>
    - <font style="color:#000000;">2.3 从数据集中学习用于分类器的未知参数</font>
+ <font style="color:#000000;">3.分类器决策</font>

## <font style="color:#000000;">（二）分类器的设计与学习</font>
### <font style="color:#000000;">基本流程图</font>![](/images/171e6b8890b2d0127eabcbbcfdf435b0.jpeg)
:::info
分类器的设计主要为图像表示-》分类模型-》损失函数-》优化算法

:::

    1. <font style="color:#000000;">图像表示：设计出的系统是需要输入图像？向量？要不要抽取特征？</font>
    2. <font style="color:#000000;">分类模型：不论模型好坏，都会输出一个预测值，通过与真实值的对比得到损失值。</font>
    3. <font style="color:#000000;">损失函数：损失值来评估当前模型的好坏</font>
    4. <font style="color:#000000;">优化算法：调整参数，降低损失值。</font>
    5. <font style="color:#000000;">循环，得到性能合理的分类器。</font>

### <font style="color:#000000;">图像表示方法</font>![](/images/f926b96bc54756ed92f7e270c6e4957e.png)
    - <font style="color:#000000;">2.1像素表示（本课程重点）</font>
    - <font style="color:#000000;">2.2全局特征表示（如GIST（频率特征）)（适用于风景类、城市建筑等大场景分类，不适用于细节分类）：从图像上抽取全局特征</font>
    - <font style="color:#000000;">2.3局部特征表示（如SIFT特征+词袋模式）</font>

2012年以前计算机视觉领域的研究都定义为特征过程，希望 在图像中找到好的特征以便识别图像

在以后的神经网络深度学习中，特征提取和分类预测已经被集合在了一起，所以只要输入图像（像素表示）即可，而不必特意县提取出特征

### <font style="color:#000000;">分类器</font>
+ <font style="color:#000000;">3.1分类器的分类（O(∩_∩)O）</font>
        * <font style="color:#000000;">近邻分类器</font>
        * <font style="color:#000000;">贝叶斯分类器</font>
        * **<font style="color:#000000;">线性分类器</font>**<font style="color:#000000;">（主讲）</font>
        * <font style="color:#000000;">支撑向量机分类器（线性分类器拓展）</font>
        * **<font style="color:#000000;">神经网络分类器</font>**<font style="color:#000000;">（主讲）</font>
        * <font style="color:#000000;">随机森林</font>
        * <font style="color:#000000;">Adaboost</font>
+ <font style="color:#000000;">3.2</font><font style="color:#DF2A3F;">学习分类器的目的</font><font style="color:#000000;">：分类器是工具，首先要会使用，其次要学习源码以便于做针对性改造。</font>

### <font style="color:#000000;">优化函数</font>
    - **<font style="color:#000000;">4.1一阶方法：梯度下降、随机梯度下降、小批量随机梯度下降</font>**<font style="color:#000000;">（</font><font style="color:#DF2A3F;">主讲</font><font style="color:#000000;">）</font>
    - <font style="color:#000000;">4.2二阶方法：</font>**<font style="color:#000000;">牛顿法</font>**<font style="color:#000000;">、BFGS、L-BFGS</font><font style="color:#000000;"> </font>_<font style="color:#000000;">(注：BFGS法(BFGS method)是一种拟牛顿法，指用BFGS矩阵作为拟牛顿法中的对称正定迭代矩阵的方法，由于BFGS法对一维搜索的精度要求不高，并且由迭代产生的BFGS矩阵不易变为奇异矩阵，因而BFGS法比DFP法在计算中具有更好的数值稳定性。)</font>_

### <font style="color:#000000;">训练过程做的事情</font>
    - <font style="color:#000000;">5.1数据集划分</font>
    - <font style="color:#000000;">5.2数据预处理</font>
    - **<font style="color:#000000;">5.3数据增强</font>**<font style="color:#000000;">（旋转、裁剪等等）（使数据集尽可能的多）</font>
    - **<font style="color:#000000;">5.4欠拟合与过拟合</font>**<font style="color:#000000;">：减小算法复杂度、使用权重正则项、使用droput正则化</font>
    - <font style="color:#000000;">5.5超参数调整</font>
    - <font style="color:#000000;">5.6模型集成</font>

### <font style="color:#000000;">五、常用的分类任务指标（45：18）</font>
<font style="color:#000000;">1.top1与top5</font>

### <font style="color:#000000;">六、线性分类器（47：30）</font>
<font style="color:#000000;">分类器设计全部知识点分布  
</font>![](/images/c63e231f6f07e0bd5afd29edbd2a498a.jpeg)



# <font style="color:#000000;">线性分类器</font>
:::info
<font style="color:#000000;">为什么学？</font>

+ <font style="color:#000000;">线性分类器形式简单、容易理解。</font>
+ <font style="color:#000000;">通过层级结构（神经网络）或者高维映射（支撑向量机）可以形成功能强大的非线性模型。（大样本王者-神经网络，小样本王者-支撑向量机）----</font><font style="color:#DF2A3F;">线性分类器是这俩的基础</font>

:::

**<font style="color:rgb(0,0,0);">线性分类器决策</font>**

## **<font style="color:rgb(0,0,0);">线性分类器</font>**<font style="color:rgb(255,0,0);">决策规则：</font>
![](/images/0ef27d830abb438bfc8fdd86e7a6df93.png)

![](/images/4a205f7a18265e07a851778dc2d10270.png)<font style="color:#000000;">  
</font>

![](/images/af0df9dac7781acd5a86d515dfad68f9.png)<font style="color:#000000;">  
</font><font style="color:#000000;">		W为c*d维的，c为分类种类 ，d为每个分类的权值向量</font>

<font style="color:#000000;">    b为c维的 ，为每个类的偏置</font>



## <font style="color:#000000;">线性分类器的决策边界</font>
1. <font style="color:#000000;">SS</font><font style="color:#DF2A3F;">如何理解线性分类器的决策边界？</font>
        * <font style="color:#000000;">首先它不是一条线，决策边界应当是一个n维向量空间（n等于wi的维数）中的i个“面”（i，既标签的个数）。每一个“面”是由方程wx + b = 0 定义的。面区分内外。（SS代入线性代数的思维区思考这一部分！！！！）</font>
        * <font style="color:#DF2A3F;">箭头方向代表分类器的正方向，沿着箭头方向距离决策面越远分数就越高。</font><font style="color:#000000;">  
</font>![](/images/9533261f47238360b232afd2f811a93f.png)
2. 理解决策平面与方程的关系

从几何上理解，线性分类器就是要找到最佳的分割线或者面，将数据分割开，以此来判断他的分类。

假设二维空间中，决策边界是一条直线wx+b=0，x实际为一个向量（x1，x2），那么wx+b代表一个直线。直线上的点是使直线为0的点。同理，对于其他点x*，代入方程实际上代表的<font style="color:#DF2A3F;">该点到直线的距离的一个度量</font>

数据点 𝑥到决策边界的有符号距离可以表示为(参考点到面的距离公式)：

![](/images/5e1aadae6ac190c8f839178e7051f854.png)	

w在高维代表的实际上可以理解为高维面的法向量，代表面的正方向，所以当一个点代入方程越大则代表分数越高

## <font style="color:#000000;">损失函数</font>
:::info
**<font style="color:rgb(0,0,0);">衡量分类器之间的好坏</font>**

:::

### <font style="color:#000000;">损失函数的定义</font>
    - <font style="color:#000000;">1.1定义：损失函数搭建了模型性能与模型参数之间的桥梁，指导模型参数优化。</font>
    - <font style="color:#000000;">1.2损失函数在工作时，一般有如下规定：一是损失函数是一个函数，用于度量给定分类器的预测值与真实值的不一致程度，其</font>**<font style="color:#000000;">输出</font>**<font style="color:#000000;">通常是一个</font>**<font style="color:#000000;">非负值</font>**<font style="color:#000000;">；二是损失函数输出的非负实值可以作为</font>**<font style="color:#000000;">反馈信号</font>**<font style="color:#000000;">来对分类器参数进行调整，以</font>**<font style="color:#000000;">降低当前示例</font>**<font style="color:#000000;">对应的</font>**<font style="color:#000000;">损失值</font>**<font style="color:#000000;">，</font>**<font style="color:#000000;">提升</font>**<font style="color:#000000;">分类器的分类效果。</font>
    - <font style="color:#000000;">1.3损失函数的</font><font style="color:#DF2A3F;">一般数学定义</font><font style="color:#000000;">：</font>
    - ![](/images/13cc8775d0ea6019722ac71b9edf5347.png)![](/images/ddad8d0520190d37d0162e3565bb0bcb.png)<font style="color:#000000;">  
</font><font style="color:#000000;">PS：W中包含了 权值向量w 和 偏移量b 。</font>



### <font style="color:#000000;">多类支撑向量机的损失（一种具体的方法）[这里并没有介绍SVM，只是说他的损失函数]</font>
+ <font style="color:#000000;">第i张图片在第j号分类标签的权值向量下的分数Sij的计算（注意体会）  
</font>![](/images/2c177014be2a1c0b837c9f66216cbb5e.png)



+ <font style="color:#000000;"> </font><font style="color:#DF2A3F;">* </font>**<font style="color:#000000;">第i个样本的多类支撑向量机损失函数定义</font>**<font style="color:#000000;">如下：（i、j含义与上文有变化，请注意理解）  
</font>![](/images/ae8848e364a55ef915c756fbbdad1c53.png)<font style="color:#000000;">为分数计算公式</font>

<font style="color:#000000;">  
</font>![](/images/419e329183daad9bc002a97cb6484fa4.png)

![](/images/e9d33549a7d5b74700694d2954c36c76.png)



<font style="color:#000000;">注意这个求和符号：i图片的损失是对除正确分类的其他预测分类损失求和的结果  
</font><font style="color:#000000;">SS：</font><font style="color:#DF2A3F;">关键在于理解if Syi > Sij + 1</font><font style="color:#000000;">；其中 +1 是降低判断的敏感度的，可以不去理解；</font><font style="color:#DF2A3F;">Syi是在现模型下，i图片应该正确的标签的得分，Sij是现模型下i图片其他标签的得分</font><font style="color:#000000;">。若Syi大，则说明模型正确；若Sij大，则说明标签与图片不符，由于是有监督的训练，图片是正确的，则说明标签贴错，则说明模型需调整。调整函数为上图所示。</font>

<font style="color:#000000;"></font>

+ <font style="color:#000000;">多类支撑向量机的损失计算的例子  
</font>![](/images/28d09130873b73a334796601a03101ec.png)<font style="color:#000000;">  
</font><font style="color:#000000;">分类器针对整个图集的损失 L ：  
</font>![](/images/1bd5c9d627f9a4bc53f98c703bfad452.png)

# <font style="color:rgb(79, 79, 79);">正则项与超参数 </font>
![](/images/5fe6388db56cbe2b50f35e7b6084d77f.png)

<font style="color:rgb(77, 77, 77);">正则损失的目的：防止模型在训练集上训练的过好。（SS防止过耦合）</font>

## <font style="color:rgb(77, 77, 77);">L2正则项损失（一个正则损失的实现方法）</font>
<font style="color:rgba(0, 0, 0, 0.75);">L2正则项的计算公式如下：</font><font style="color:rgba(0, 0, 0, 0.75);">  
</font>![](/images/37d4496953d5acee1b83201bab29213b.jpeg)

<font style="color:rgba(0, 0, 0, 0.75);">4.2 L2正则项在i=1（只有一个标签）时的例子</font>

<font style="color:rgba(0, 0, 0, 0.75);">  
</font>![](/images/ea53bb1b972436f1c6d82c173ad80652.jpeg)

    - <font style="color:rgba(0, 0, 0, 0.75);">4.3 L2正则损失对大数权值进行惩罚，喜欢分散权值，鼓励分类器讲所有维度的特征都应用起来，而不是强烈的依赖其中少数几维 特征。（正则项的使用，使得模型在训练时有了偏好！）</font>

### <font style="color:rgba(0, 0, 0, 0.75);">常用的正则项损失  
</font>![](/images/2d72062806e09240bb155407fc5c91cd.jpeg)
# <font style="color:rgb(79, 79, 79);">二、优化与常见算法（对应视频中7.8两点）</font>
## <font style="color:rgb(79, 79, 79);">（一）什么是优化</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.参数优化定义：参数优化是机器学习的核心步骤之一，它利用损失函数的输出值作为反馈信号来</font>**<font style="color:rgba(0, 0, 0, 0.75);">调整分类器参数</font>**<font style="color:rgba(0, 0, 0, 0.75);">，以提升分类器</font>**<font style="color:rgba(0, 0, 0, 0.75);">对训练样本</font>**<font style="color:rgba(0, 0, 0, 0.75);">的预测性能。</font>

## <font style="color:rgb(79, 79, 79);">（二）常见算法</font>
### <font style="color:rgb(77, 77, 77);">1.直接方法</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">什么是直接方法：</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">正则损失函数如下图所示 </font>![](/images/a261f612d8808a1c8a49288f10ba1e3b.png)
    - <font style="color:rgba(0, 0, 0, 0.75);">直接方法如下图所示</font>![](/images/956effdcdef1ee7913eb1efbcb89e20b.png)<font style="color:rgba(0, 0, 0, 0.75);">  
</font><font style="color:rgba(0, 0, 0, 0.75);">由于W是一组包含 wi 和 bi 的矩阵，所以实际上列为方程组应写为：</font>![](/images/c87f223ebdc427db537094715b999378.png)<font style="color:rgba(0, 0, 0, 0.75);">  
</font><font style="color:rgba(0, 0, 0, 0.75);">若能解出这样一组解，则有一组优秀的参数W。</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">目标：损失函数L是一个与参数W有关的函数，优化的目标就是找到使损失函数L达到</font>**<font style="color:rgba(0, 0, 0, 0.75);">最优</font>**<font style="color:rgba(0, 0, 0, 0.75);">的那组解。（注意表达，最优，最理想的状态当然是L=0,既不会犯错，但很难。）</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">为什么出现直接法？答：因为令L=0，想找出这样一组解基本是不可能的。并且由于该模型是针对特定训练集的，L=0得到的这组参数很可能存在过耦合问题，因此也是找出的这样的解其意义也是值得商榷的。</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">1.5面临的困难：通常L的形式是十分复杂的，很难从这个等式中直接解除W。</font>

### <font style="color:rgb(77, 77, 77);">2.梯度下降算法-一种简答而高效的算法</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">2.1梯度：</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">梯度的本意是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。通过数学工具，梯度的向量是可求的。在函数L=…中，变量集合W在某一点上的梯度是渴求的，则梯度的负方向，就是L降低的方向。</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">2.2整个机器学习中</font>**<font style="color:rgba(0, 0, 0, 0.75);">最重要的概念</font>**<font style="color:rgba(0, 0, 0, 0.75);">—</font>**<font style="color:#DF2A3F;">步长/学习率</font>**<font style="color:rgba(0, 0, 0, 0.75);">：使用负梯度方法，确定好w的调参方向。每一步调多远，是极为重要的。</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">2.3计算方法的抽象（思路）：</font>
    - ![](/images/034bda827b69d5f824eeac1276c3b638.png)<font style="color:rgba(0, 0, 0, 0.75);">  
</font><font style="color:rgba(0, 0, 0, 0.75);">下面就要解决：计算梯度（损失，训练样本，权值）。</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">2.4计算梯度方法之</font>**<font style="color:rgba(0, 0, 0, 0.75);">数值法</font>**<font style="color:rgba(0, 0, 0, 0.75);">：计算量大且不精确，通常不用。（就是求极限）（仅需知道有这么一种方法）</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">2.5梯度计算方法之</font>**<font style="color:rgba(0, 0, 0, 0.75);">解析法</font>**<font style="color:rgba(0, 0, 0, 0.75);">：就是求导啊…</font>
        * <font style="color:rgba(0, 0, 0, 0.75);">2.5.1项目损失函数的出现：</font>![](/images/c084888700c13eb5cb7fcf1b10d0b9d7.png)<font style="color:rgba(0, 0, 0, 0.75);"> </font><font style="color:rgba(0, 0, 0, 0.75);">+ 2.5.2损失函数的计算（既一个分段函数的求导）  
</font>![](/images/e931d5d06da4971a35a15fd0b6014d22.png)
        * <font style="color:rgba(0, 0, 0, 0.75);">2.5.3求出Li（W）的导函数之后，再取平均值求L(W)的导数  
</font>![](/images/7ab4959c9b82c326e70ea5c0295b1642.png)
        * <font style="color:rgba(0, 0, 0, 0.75);">2.5.4这是问题就出现了，当N很大时，权值的梯度计算量很大。（在有1,000,000个样本，10个标签的情况下，要进行10,000,000次计算才能更新一次参数，效率实在是难以接受）</font>

### <font style="color:rgb(77, 77, 77);">3.随机梯度下降算法：其随机体现在每次更新选择一个随机样本。</font>![](/images/5cb9f53d17f9fa882ebf2d6848c8ada7.png)
### <font style="color:rgb(77, 77, 77);">4.小批量梯度下降算法：每次样本m个。（超参数）</font>
+ ![](/images/e5f4e0f55e16e3016ab309ecdede141e.png)<font style="color:rgb(77, 77, 77);">  
</font>**<font style="color:#DF2A3F;">注意上面几个名词。</font>**
+ <font style="color:rgb(77, 77, 77);">5.总结上面三种梯度下降算法、随机梯度下降算法、小批量梯度下降算法。  
</font>![](/images/128236e90808be700d6b992e1e0c46b4.png)<font style="color:rgb(77, 77, 77);">  
</font><font style="color:rgb(77, 77, 77);">注：讲了损失函数的计算，但没有讲怎么调参。</font>

# <font style="color:rgb(79, 79, 79);">三、数据集划分</font>
## 各数据集的作用
P1:训练集的作用

拟合模型，调整网络权重。

P2:验证集的作用

作用1：快速调参，也就是通过验证集我们可以选择超参数（网络层数、网络节点数、迭代次数epoch、学习率learning rate、优化器）等。

作用2：选择超参数，为了让我们的模型在测试集表现得更好，调参是不可避免地一部分，如果把测试集当验证集，调参去拟合测试集合，是不可行地，这相当于作弊。

作用3：监控模型是否正常

验证集的重要性：

如果没有设置验证集，我们通常得等到测试集才可以知道我们模型真正得实力，然后再来调整参数，这样子时间代价较高，通过验证集我们可以训练几个epoch后查看模型的训练效果及我们的网络是否出现异常，然后决定怎么调整我们的超参数。

P3:测试集的作用

仅仅用来评估模最终模型的泛化能力，确认网络的实际预测能力。

![](/images/48cbaf973b5cb8be6dd04c0f193b2cdb.png)

+ 神经网络在网络结构确定的情况下,有两部分影响模型最终的性能,一是普通参数(比如权重w和偏置b),另一个是超参数(例如学习率,网络层数).普通参数我们在训练集上进行训练,超参数我们一般人工指定(比较不同超参数的模型在验证集上的性能)<font style="color:#DF2A3F;">.那为什么我们不像普通参数一样在训练集上训练超参数呢?</font>(花书给出了解答)

一是超参数一般难以优化(无法像普通参数一样通过梯度下降的方式进行优化).

二是超参数很多时候不适合在训练集上进行训练,例如,如果在训练集上训练能控制模型容量的超参数,这些超参数总会被训练成使得模型容量最大的参数(因为模型容量越大,训练误差越小),所以训练集上训练超参数的结果就是模型绝对过拟合.

+ 正因为超参数无法在训练集上进行训练,因此我们单独设立了一个验证集,用于选择(人工训练)最优的超参数.因为验证集是用于选择超参数的,因此验证集和训练集是独立不重叠的.
+ 测试集是用于在完成神经网络训练过程后,为了客观评价模型在其未见过(未曾影响普通参数和超参数选择)的数据上的性能,因此测试与验证集和训练集之间也是独立不重叠的,而且测试集不能提出对参数或者超参数的修改意见,只能作为评价网络性能的一个指标.



至此,我们可以将神经网络完整的训练过程归结为一下三个步骤：

Step1:训练普通参数。

在训练集(给定超参数)上利用学习算法,训练普通参数,使得模型在训练集上的误差降低到可接受的程度(一般接近人类的水平)。

Step2:'训练’超参数。

在验证集上验证网络的generalization error(泛化能力),并根据模型性能对超参数进行调整。

Step3:重复1和2两个步骤,直至网络在验证集上取得较低的generalization error。此时完整的训练过程结束.在完成参数和超参数的训练后,在测试集上测试网络的性能。



# K折交叉验证
![](/images/f364dcca6738967d716631d019af105e.png)

![](/images/8be3aee308b32ea637afa5e1846989a8.png)

![](/images/38b5ed62b6bfd453cd505106c059ded1.png)

# 预处理
##  去均值
![](/images/0d100f0d1e347f9bf49b33d7f2f1266a.png)

<font style="color:#DF2A3F;">各维度都减对应维度的均值</font>，使得输入数据各个维度都中心化为0，进行去均值的<font style="color:#DF2A3F;">原因</font>是因为如果不去均值的话会容易拟合。

![](/images/836bdbbc4563249c17bae7a8c0e74013.png)

右图为去均值之后的效果。

## 归一化
均值方差归一化（标准化），一般是把所有数据归到均值为0，方差为1的分布中。即确保最终得到的数据均值为0，方差为1

![](/images/68917f48c71b6181ec4cf3a19ef4740c.png)

μ 为样本的平均值，S 为样本的标准差



![](/images/3e585b17b69c4b163ed626f6fb1eb14d.png)![](/images/34d283f2418605c8bf63d85ca6a365b6.png)

## 去相关 与 PCA白化
![](/images/be8e46699be1ca8ca86d2f1815c1ea69.png)

### 去相关
把特征之间的相关性降低，数据的协方差矩阵变成对角阵，且中心为0

![](/images/96a4ca4d3280f74604e6d2ffcd170fc8.png)![](/images/6c1deaa9b5bb68741717d26ca1f10aec.png)![](/images/49a7fed979e063713809a149486ba2f7.png)

通过协方差矩阵可以求得特征向量u1、u2，然后把每个数据点，投影到这两个新的特征向量，得到进行坐标如下：![](/images/04e9516ecae4655a7f539e0c8e13bb6e.png)

### PCA 白化（Principal Component Analysis whitening）
###  
:::info
在数据预处理中，协方差矩阵成为单位矩阵的过程称为“白化”（whitening）。这是PCA白化（PCA whitening）中的一个步骤  

:::

  
 对去相关后的数据进行标准化处理，使得每个特征的方差为1。具体方法是将每个特征除以其对应的标准差（即特征值的平方根）。  

 经过白化后的数据，其协方差矩阵将成为单位矩阵（identity matrix），即对角线上的元素为1，非对角线上的元素为0。这意味着白化后的特征不仅不相关，而且具有相同的方差（即单位方差）。  

![](/images/e6355a8dbc4ffc8574c12ed2f5a3754a.png)
