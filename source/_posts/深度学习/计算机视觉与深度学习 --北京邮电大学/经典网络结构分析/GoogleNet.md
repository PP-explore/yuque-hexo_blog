---
title: GoogleNet
date: '2024-12-30 17:03:13'
updated: '2025-08-22 16:14:48'
---
N = (W − F + 2P )/S+1

![](/images/cc5e0e962ca2beebfcf3d60ab1dd78cd.png)

对上图做如下说明：

1 . 显然GoogLeNet采用了模块化的结构，方便增添和修改；

2 . 网络最后采用了average pooling来代替全连接层，想法来自NIN,事实证明可以将TOP1 accuracy提高0.6%。但是，实际在最后还是加了一个全连接层，主要是为了方便以后大家finetune；

3 . 虽然移除了全连接，但是网络中依然使用了Dropout ;

4 . 为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度。文章中说这两个辅助的分类器的loss应该加一个衰减系数，但看caffe中的model也没有加任何衰减。此外，实际测试的时候，这两个额外的softmax会被去掉。

#### 创新点
##### Inception结构
Inception 结构的主要思路是怎样用密集成分来近似最优的局部稀疏结构。  
作者首先提出下图这样的基本结构：



![](/images/62bcca1ddb6bf4c6dfa8776657d6720f.png)对上图做以下说明：

1 . 采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合；

2 . 之所以卷积核大小采用1、3和5，主要是为了方便对齐。设定卷积步长stride=1之后，只要分别设定pad=0、1、2，那么卷积之后便可以得到相同维度的特征。

3 . 3×3 max pooling 可理解为非最大化抑制。文章说很多地方都表明pooling挺有效，所以Inception里面也嵌入了。保留且加强了原图中比较重要的信息。

4 . 网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例也要增加。

5 . 1×1 3×3 5×5卷积，及3×3max pooling，通过设定合适的pad都会得到相同维度的特征，然后将这些特征直接拼接在一起。



但是，使用5x5的卷积核仍然会带来巨大的计算量。 为此，文章借鉴NIN2，采用1x1卷积核来进行降维。

例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256。其中，卷积层的参数为128x5x5x256。假如上一层输出先经过具有32个输出的1x1卷积层，再经过具有256个输出的5x5卷积层，那么最终的输出数据仍为100x100x256，但卷积参数量已经减少为128x1x1x32 + 32x5x5x256，大约减少了4倍。



具体改进后的Inception Module如下图：

![](/images/96148ff052241f08e1fa0a2335ea32e6.png)

![](/images/c2c36f17c809a18fb32f4dcbbfb9d7fa.png)![](/images/56136d4d075f68ac6aad522b4bcd1ffa.png)

##### 平均池化+去除两个全连接层![](/images/d0207a1fdac7cc29811acbc437ed8e48.png)辅助分类器
防止梯度消失。![](/images/dc8405c433d9a9fce266f4b41715fa17.png)![](/images/3ba83b7e046532d6fde171bca4a9bdcb.png)

#### 思考
##### 问题1 :平均池化向量化与直接展开向量化有什么区别?![](/images/2e83e09ed4cfa3cecc2a38341488fa4d.png)特征响应图中位置信息不太重要，平均池化，忽略位置信息，可以很大节省计算量。


问题2: 利用1 x1卷积进行压缩会损失信息吗?

不会，假设图像或特征响应图深度通道为64，其中记录信息的只有少数，对应的向量非常稀疏，且其后的每个卷积核（深度通道也为64）都作用在这64个通道上。 经过压缩，并不会影响图像原始信息的记录。![](/images/8ce1894c6bce27e4f8b7c38595a0d4bb.png)

