---
title: 第六课  全连接神经网络下
date: '2024-07-11 19:55:13'
updated: '2025-08-22 18:00:47'
categories:
  - 人工智能
tags:
  - 深度学习
cover: /images/custom-cover.jpg
recommend: true
---
# <font style="color:rgb(79, 79, 79);">一、欠拟合与过拟合</font>
## <font style="color:rgb(79, 79, 79);">（一）概念（略）</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.欠拟合：。。。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">2.过拟合：。。。</font>

## <font style="color:rgb(79, 79, 79);">（二）L2正则化</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.</font>**<font style="color:rgba(0, 0, 0, 0.75);">次优方案-正则化</font>**<font style="color:rgba(0, 0, 0, 0.75);">：调节模型允许存储的信息量或者对模型语序存储的信息加以约束，该方法也称为正则化。</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">1.1调节模型大小</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">1.2约束模型权重，既权重正则化（常用的有L1、L2正则化）</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">1.3</font>**<font style="color:rgba(0, 0, 0, 0.75);">随机失活（dropout)</font>**
+ <font style="color:rgba(0, 0, 0, 0.75);">2.使用L2正则化的损失函数为：（红字为L2正则项）  
</font>![](/images/3ffc010cb394ea78bc4c0f636045a062.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">3.L2正则损失对于大数值的权值向量进行严厉惩罚，鼓励更加分散的权重向量，使模型倾向于使用所有输入特征做决策，此时的模型泛化性能好。</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">3.1 SS 泛化性能好，是指模型在验证集和</font>**<font style="color:rgba(0, 0, 0, 0.75);">真实情况中</font>**<font style="color:rgba(0, 0, 0, 0.75);">的性能更好。</font>

## <font style="color:rgb(79, 79, 79);">（三）随机失活dropout</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.随机失活：让隐藏的神经元以一定</font>**<font style="color:#DF2A3F;">概率</font>**<font style="color:rgba(0, 0, 0, 0.75);">不被激活。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">2.【概念、超参数】随机失活比率（Dropout ratio）：是被设定为0的特征所占的比例，通常在0.2—0.5范围内。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">3.实现方式：训练过程中，对某一层使用Dropout，就是随机将该层的一些输出舍弃（输出值设置为0），这些被舍弃的神经元就好像被网络删除了一样。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">4.随机失活为什么能够防止过拟合？</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">解释1：能够降低模型容量（不解释）</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">解释2：鼓励权重分散（从而实现与正则化一样的效果）</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">解释3：</font>**<font style="color:rgba(0, 0, 0, 0.75);">Dropout可以看作模型的集成</font>**
+ <font style="color:rgba(0, 0, 0, 0.75);">5.随机失活在使用中存在的问题（此例中使用dropout系数0.5）</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">5.1借用数学工具，期望值的计算：  
</font>![](/images/fa80fb1f8ae7bf7a269334f38a0a6fc3.png)
    - <font style="color:rgba(0, 0, 0, 0.75);">5.2可以发现，用dropout训练时，训练阶段的模型对训练集所有图的</font>**<font style="color:rgba(0, 0, 0, 0.75);">输出的值</font>**<font style="color:rgba(0, 0, 0, 0.75);">的数学期望，系统性的低于实际使用中（这阶段没有随机失活）的输出值的数学期望。（至于具体低多少，与失活比率相同）</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">应用示例（python）（具体流程57：00）  
</font>![](/images/e3cb8f42fe5a289564ea58f76826725d.png)

# <font style="color:rgb(79, 79, 79);">二、神经网络中的超参数</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.网络结构：隐层神经元个数、网络层数、非线性单元选择等</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">2.优化相关：学习率、dropout比率、正则项强度等</font>

## <font style="color:rgb(79, 79, 79);">（一）学习率的调整</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.基于经验的手动调整（常用）  
</font><font style="color:rgba(0, 0, 0, 0.75);">通过尝试不同的固定学习率，如3、1、0.5、0.1、0.05、0.01、0.005，0.005、0.0001、0.00001等，观察迭代次数和loss的变化关系，找到loss下降最快关系对应的学习率。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">2.基于策略的调整</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">2.1 fixed 、exponential、polynomial</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">2.2自适应动态调整：adadelta、adagrad、ftrl、momentum、RMSProp、sgd</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">3.超参数选择时的</font>**<font style="color:rgba(0, 0, 0, 0.75);">标尺空间</font>**<font style="color:rgba(0, 0, 0, 0.75);">（无论手动还是随机选择）  
</font>![](/images/7a55bd9b8713055c74205e63f40656d7.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">3.1建议（经验）：在</font>**<font style="color:rgba(0, 0, 0, 0.75);">对数空间</font>**<font style="color:rgba(0, 0, 0, 0.75);">上进行随机采样更合适。</font>

<font style="color:rgb(77, 77, 77);">（79：20-结束）是对以上三章内容的串讲，有较好的连贯性和总结性，建议反复听。</font>

**<font style="color:rgb(34, 34, 38);">文章知识点</font>**
