---
title: 第四课  全连接神经网络 上
date: '2024-07-06 14:58:22'
updated: '2025-08-22 16:15:19'
---
# <font style="color:rgb(79, 79, 79);">一、全景预览（共11个问题）</font>
![](/images/90e6525b86c6da4405f641d142bad23e.png)<font style="color:rgb(77, 77, 77);">  
</font><font style="color:rgb(77, 77, 77);">注：图像表示问题一笔带过</font>

# <font style="color:rgb(79, 79, 79);">二、多层感知器（又称全连接神经网络）</font>
#### <font style="color:rgb(79, 79, 79);">（一）由线性分类器引出（层层套娃）</font>
+ <font style="color:rgb(77, 77, 77);">1.全连接神经网络，可以看作</font><font style="color:#DF2A3F;">多个线性分类器</font><font style="color:rgb(77, 77, 77);">经过非线性连接以后的结果。</font>

<font style="color:rgb(77, 77, 77);">  
</font>![](/images/3899f02cabde3aba006540432138240f.png)

    - **<font style="color:rgba(0, 0, 0, 0.75);">其中max（0，…）称为激活函数</font>**
+ <font style="color:rgb(77, 77, 77);">2.一层的线性分类器，</font><font style="color:#DF2A3F;">无论实现还是训练都已经很麻烦，为什么还有分层？</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">2.1在只有一层的情况下，我的线性分类器的权值矩阵W只能是10×3072的二维矩阵。（10由类别标签个数确定，3072由图片格式决定）</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">2.2 SS 从线性代数角度讲，在两层情况下，矩阵乘法a×b是10×3072时，a的列数和b的行数只要相等就可以是任何数。比如a是10×100矩阵，b是100×3072矩阵，则a×b仍是10×3072矩阵。由此回到分类其中，a=W2，b=W1，仍能保证，输入3072维的x图像，输出10个类别标签的得分。</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">2.3 SS </font><font style="color:#DF2A3F;">可行性有了，优越性体现在何处？</font><font style="color:rgba(0, 0, 0, 0.75);">空泛的说，你更自由了，更自由必然是好的。具体来说，你可以在一开始分成更多类，分的越精准，最后输出的错误越少。</font>
+ <font style="color:rgb(77, 77, 77);">3.线性可分与线性不可分</font>
    - ![](/images/69a201e9c5538705c52a1efc6f1f5e18.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">线性可分由线性分类器输出。  
</font>![](/images/e6d6cd40679cd8803164fb04ab6c0ca2.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">线性不可分由全连接神经网络输出。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">二维平面上，线性可分都有难度，请自行想象3072维空间中，是否可能画出线性的超平面，用于划分不同的标签？</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">4.绘制与命名</font>![](/images/d9dff35d0097c136af90881e56798f23.png)

# <font style="color:rgb(79, 79, 79);">三、激活函数</font>
## <font style="color:rgb(77, 77, 77);">四种常用激活函数  
</font>![](/images/7cbdb84aff465fd6bcea10d0ae06897c.png)
## <font style="color:rgb(77, 77, 77);">sigmoid函数也叫Logistic函数，它可以将一个实数映射到(0,1)的区间，可以</font><font style="color:#DF2A3F;">用来做二分类</font><font style="color:rgb(77, 77, 77);">。</font>
+ <font style="color:rgb(77, 77, 77);">优点：平滑、易于求导。</font>
+ <font style="color:rgb(77, 77, 77);">缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">2.1Sigmoid函数由下列公式定义：</font>
    - ![](/images/564e95fa08bd0348219a9aec39799ef5.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">2.2其对x的导数可以用自身表示：</font>
    - ![](/images/a002d6f58c6ece25aa6c9c8a01af463e.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">2.3Sigmoid函数的图形如S曲线</font>
    - ![](/images/5d2afefa2d4a2690d09a46705268f972.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">2.4比较活跃的取值范围[-5,5]，在之外，函数值就非常接近0，或接近1。</font>

## <font style="color:rgb(77, 77, 77);">tanh函数</font>
## <font style="color:rgb(77, 77, 77);">ReLU函数</font>
## <font style="color:rgb(77, 77, 77);">Leaky ReLU函数</font>
# <font style="color:rgb(79, 79, 79);">四、网络结构设计</font>
## <font style="color:rgb(79, 79, 79);">（一）提出问题</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.用不用隐层？用几个隐层？（深度设计）</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">2.每个隐层设计多少个神经元？（宽度设计）</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">3.本节不谈，但是也很重要的方面，选择什么样的激活函数？</font>

## <font style="color:rgb(79, 79, 79);">(二）模型的复杂程度要与现实问题的复杂程度相匹配</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.一个例子，神经元个数越多，能画出越复杂的分界面，但过复杂的分界面也有过拟合的问题  
</font>![](/images/2640879c9b3323c1865b8a785a2671ba.png)
+ ![](/images/e2cbeb2201b13c900063f3ff6b3513f7.png)

# <font style="color:rgb(79, 79, 79);">五、损失函数之SOFRMAX与交叉熵</font>
## <font style="color:rgb(79, 79, 79);">（一）什么叫SOFTMAX</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.为什么？答：在使用模型时，一张图对应10个标签的不同分数，分数最高的标签，最可能对。但每个标签可能正确的概率是多少要如何表示？使用SOFTMAX方法表示。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">2.怎么算？（视频34：55）详细步骤可回顾</font>
+ ![](/images/ecb01136c5e8d9cc0514c885c6ce4dee.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">3.一个实例</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">最后得出的0.21bird、0.01cat、0.78car是一个概率分布</font>![](/images/611d1173bf6a09195491300faab4338d.png)

## <font style="color:rgb(79, 79, 79);">（二）交叉熵损失</font>
### <font style="color:rgba(0, 0, 0, 0.75);">1.如何度量现在的分类器输出的该楼层分布与预测值之间的距离？</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.面临的困难：多类支撑向量机的损失函数，度量的是图片在各类标签下的得分（Sij与Yj）；但现在需要度量的是</font>**<font style="color:rgba(0, 0, 0, 0.75);">SOFTMAX的概率分布结果</font>**<font style="color:rgba(0, 0, 0, 0.75);">与</font>**<font style="color:rgba(0, 0, 0, 0.75);">正确的概率分布结果</font>**<font style="color:rgba(0, 0, 0, 0.75);">之间的距离。</font>![](/images/0827c69b46b5c8b5e8f799c48d48bcf8.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">2.引入新的数学工具：熵、交叉熵、相对熵</font>
+ ![](/images/e70edfddd8ddf70b57ceabc09a284e93.png)
    - <font style="color:rgba(0, 0, 0, 0.75);">2.1信息熵（扩展自百度百科）计算公式：</font>
        * ![](/images/23a04c0766186849c00b9bc4777d5a32.png)
        * _<font style="color:rgba(0, 0, 0, 0.75);">其中，x表示随机变量，与之相对应的是所有可能输出的集合，定义为符号集,随机变量的输出用x表示。P(x)表示输出概率函数。变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大。</font>_
        * _<font style="color:rgba(0, 0, 0, 0.75);">当P为均匀分布时，熵最大。如（1/3，1/3，1/3）</font>_
        * <font style="color:rgba(0, 0, 0, 0.75);">或者写为（SS:K可以理解为系数，注意负号）</font>
        * ![](/images/212fd689592cad9a5f640e86f900d044.png)
    - <font style="color:rgba(0, 0, 0, 0.75);">2.2交叉熵</font>
        * <font style="color:rgba(0, 0, 0, 0.75);">主要用于度量两个概率分布间的差异性信息。</font>
        * <font style="color:rgba(0, 0, 0, 0.75);">计算公式：</font>
        * ![](/images/85ff29b003165d3269922f9ed7fde948.png)
        * _<font style="color:rgba(0, 0, 0, 0.75);">其中</font>__<font style="color:#DF2A3F;">p,q为一个样本集中两个概率分布</font>__<font style="color:rgba(0, 0, 0, 0.75);">，其中p为真实分布，q为非真实分布。</font>_
        * _**<font style="color:#DF2A3F;">三者的关系</font>**_
    - ![](/images/4b3e9e11a33a69d15ae2396bc7d99cab.png)
    - <font style="color:rgba(0, 0, 0, 0.75);">2.3</font>**<font style="color:rgba(0, 0, 0, 0.75);">应用-计算上图中的交叉熵损失：</font>**<font style="color:rgba(0, 0, 0, 0.75);">  
</font>![](/images/3a1afb112c8c7718e8887971c6ef1b70.png)
        * <font style="color:rgba(0, 0, 0, 0.75);">在</font><font style="color:#DF2A3F;">真实分布为one-hot形式时</font><font style="color:rgba(0, 0, 0, 0.75);">，部分等于0的项</font><font style="color:#DF2A3F;">可以直接化简</font><font style="color:rgba(0, 0, 0, 0.75);">，交叉熵损失化简为：</font>
        * ![](/images/0f53f511bc1ca65c6d2d61de680f5e7c.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">3.我们事实上是使用</font>**<font style="color:rgba(0, 0, 0, 0.75);">相对熵</font>**<font style="color:rgba(0, 0, 0, 0.75);">（也称KL散度）衡量损失的，但是由于</font>**<font style="color:rgba(0, 0, 0, 0.75);">计算</font>**<font style="color:rgba(0, 0, 0, 0.75);">上q、p概率中的p是one-hot形式的，这使得</font>**<font style="color:rgba(0, 0, 0, 0.75);">相对熵等于交叉熵</font>**<font style="color:rgba(0, 0, 0, 0.75);">，所以我们称呼这种用交叉熵来度量的损失，为</font>**<font style="color:rgba(0, 0, 0, 0.75);">交叉熵损失</font>**<font style="color:rgba(0, 0, 0, 0.75);">。交叉熵在计算过程中，由于p是one-hot形式，故又可化简为上图中的极其简单的式子。最终的式子使用起来非常简单，但一定要记住其背后</font>**<font style="color:rgba(0, 0, 0, 0.75);">有</font>**<font style="color:rgba(0, 0, 0, 0.75);">深刻的意义，有</font>**<font style="color:rgba(0, 0, 0, 0.75);">余力</font>**<font style="color:rgba(0, 0, 0, 0.75);">的话去理解其引入的数学工具的计算方法。</font>

# <font style="color:rgb(79, 79, 79);">六、交叉熵损失与多类支撑向量机损失的比较</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.写在开头，注意区分两种损失字面上的不同。交叉熵损失是，以交叉熵这种数学工具度量的损失。多类支撑向量机损失是，这种名为多类支撑向量机的分类器的通过权值打分再比较再求平均值后计算出的损失。</font>![](/images/2c327f9d4bd99e4a54e4a5b0ae6416d7.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">2.使用交叉熵损失进行训练时，会出现这样一种情况，我们的多类支撑向量机损失没有变，但是训练精度一直在提升。其背后就是交叉熵损失在降低。  
</font>![](/images/c44975a35f36c665dbd88137435f7e16.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">3.交叉熵损失，要求别人小</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">多类支撑损失，要求自己至少比别人大1。</font>

# <font style="color:rgb(79, 79, 79);">七、计算图与反向传播</font>
## <font style="color:rgb(79, 79, 79);">（一）什么是计算图</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.定义：计算图是一种有向图，它用来表达输入、输出以及中间变量之间的计算关系，图中的</font>**<font style="color:rgba(0, 0, 0, 0.75);">每个节点</font>**<font style="color:rgba(0, 0, 0, 0.75);">对应着一种数学运算。</font>

![](/images/4c5e6fe0959cdd7487b2fafd96f730c7.png)

+ <font style="color:rgba(0, 0, 0, 0.75);">2.计算图的前反向计算</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">使用数学工具—链式求导法则—实现梯度的反向计算</font>
+ ![](/images/71c4374a2b7e5fe421b909228befa471.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">3.计算图总结</font>

![](/images/0c87da2bd75eaadff80f26c048c90f50.png)

    - <font style="color:rgba(0, 0, 0, 0.75);">3.4梯度的方向传到整个过程（66：48-69：50），讲的非常零基础，都是高数的基础内容，但是很容易把人绕迷糊了。  
</font>![](/images/9e10170a74e86d67607866c6efc8bc64.jpeg)<font style="color:rgba(0, 0, 0, 0.75);">  
</font><font style="color:rgba(0, 0, 0, 0.75);">反向传播过程  
</font>![](/images/73860aa60095733a90245cb1ceb2ffd1.jpeg)
+ <font style="color:rgba(0, 0, 0, 0.75);">4.顺便强调sigmoid函数，为什么经常使用它？看sigmoid函数的导数形式即可。方便计算啊！  
</font>![](/images/90bf711c661fb96b9d9f069994e8a426.jpeg)

从上方推导可知：sigmod函数的导数为![](/images/d790161a30c0b362161685ad649f4bc1.png)可以将sigmod中的多个运算作为一个整体来参与求梯度。增大求梯度的颗粒度。

+ <font style="color:rgba(0, 0, 0, 0.75);">5.介绍i中计算途中常见的们单元及其导数（其中拷贝门要多留心，反向传播是导数要加起来）  
</font>![](/images/38691144c494c2d46b10a73e050e0500.jpeg)

