---
title: 第五课  全连接神经网络中
date: '2024-07-08 17:59:51'
updated: '2025-09-16 20:41:46'
categories:
  - 人工智能
tags:
  - 深度学习
  - 计算机视觉与深度学习
cover: /images/custom-cover.jpg
recommend: true
---
## <font style="color:rgb(79, 79, 79);">一、再谈激活函数</font>
## <font style="color:rgb(79, 79, 79);">（一）梯度消失问题</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.从sigmoid函数的导数引出梯度消失问题</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">sigmoid函数及其导函数如图所示</font>
    - ![](/images/570b13d31bc1f2f5534b7c4de104984e.png)
    - <font style="color:rgba(0, 0, 0, 0.75);">因此sigmoid现在不常用了</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">2.梯度消失是神经网络训练中非常致命的问题，其本质是由于数学工具—链式求导法则—的惩罚特性导致的。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">3.梯度爆炸：断崖处梯度乘以学习率后会是一个非常大的值，从而“飞”出了合理区域，最终导致算法不收敛。</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">解决方法：把沿梯度方向前进的步长限制在某个值内，就可以避免“飞”出了，这个方法也成为梯度裁剪。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">4.同样的，tanh（双曲正切函数）也有局部梯度特征不利于网络梯度流的方向传递（既梯度消失问题）。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">5.ReLU函数方便使用  
</font>![](/images/c9ed47bb1238d07cc72fccc930ad7f04.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">ReLU函数的局部特性方面，当输入大于0时，局部梯度永远不会为0，比较有利于梯度流的传递。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">6.Leakly ReLU激活函数：好处是梯度永远不为0，而且梯度很好计算。</font>![](/images/f1843bcfa8beb16134df308f0a08daab.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">7.激活函数如何选择？尽量选择ReLU或者Leakly ReLU函数，相对于Sigmoid/tanh，ReLU函数或者Leakly ReLU函数会让梯度流更加顺畅，训练过程手收敛得更快。</font>

## <font style="color:rgb(79, 79, 79);">二、动量法与自适应梯度</font>
[小白零基础学习：详解梯度下降算法：完整原理+公式推导+视频讲解_标准梯度下降算法 损函数-CSDN博客](https://blog.csdn.net/zhouaho2010/article/details/102756411)

[深度学习各类优化器详解（动量、NAG、adam、Adagrad、adadelta、RMSprop、adaMax、Nadam、AMSGrad）_动量优化器-CSDN博客](https://blog.csdn.net/qq_42109740/article/details/105401197)

### 回忆线性分类及损失基本的概念
![](/images/07f53944cf9dd6e5e12707af3af7e26b.png)

![](/images/4c968d1d6a12c2494cabf3b2147d83d2.png)

![](/images/e85731c028ed17293cbbab8014828e1b.png)

![](/images/d4f1f1fdc5e609b6d66d4c81721cbee9.png)

![](/images/660dc7f40129678a264a5d320dae65c9.png)

![](/images/359f607f45617d20c09ca98e42c29152.png)

理解公式各元素：

+ ![image](/images/3eb176959a311a13f9e1a8a127c8d24d.svg)为数据集的一个样本，也就是图片集的一个图片，其为一个向量，向量的维数是图片RGB像素的大小。
+ 如在线性回归中，![image](/images/3eb176959a311a13f9e1a8a127c8d24d.svg)维数为一个样本的特征数，表示为![image](/images/5877526ec933fb9aaf939eee85f2a688.svg)

### 梯度下降算法改进
#### <font style="color:rgba(0, 0, 0, 0.75);">批量梯度下降法BGD（Batch Gradient Descent）</font>
<font style="color:rgba(0, 0, 0, 0.75);"> 批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新，这个方法就是前面讲的线性回归的梯度下降算法。</font>  
![](/images/87d5ec6f3634c94c6a1098c9b2ca5ec1.png)

#### 随机梯度下降法SGD（Stochastic Gradient Descent）
　　　　随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的N个样本的数据，而是仅仅选取一个样本j来求梯度。对应的更新公式是：

　　　![](/images/472733a30c52f0cb5f0a7480ce9a151a.png)

　　　那么，有没有一个中庸的办法能够结合两种方法的优点呢？有！这小批量梯度下降法MBGD。

#### 小批量梯度下降法MBGD（Mini-batch Gradient Descent）
　　小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于N个样本，我们采用x个样本来迭代，1<x<N。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。对应的更新公式是：

![](/images/2ccedf9b0b7c816d3f6b0172d8c15b1e.png)

### <font style="color:rgb(79, 79, 79);">（一）动量法（ momentum）</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.为什么需要动量法？答：当一个损失函数在一个方向上变化迅速，在另一个方向上变化缓慢时，在向答案收敛时，计算中各个步骤会在总体上表现为在快的方向来回震动，在慢的方向进展缓慢。使用我们已有的梯度下降方法作为工具，不能很好解决这一问题。</font>
+ ![](/images/e0cb71d1da3bbb83772d21e29e106d69.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">2.改进思想：利用累加历史梯度信息，更新梯度。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">3.两种方法伪代码对比  
</font>![](/images/8d33d9753bf3350c883fa2986ee94394.png)<font style="color:rgba(0, 0, 0, 0.75);">  
  
</font><font style="color:rgba(0, 0, 0, 0.75);">*3.1 </font>_**<font style="color:rgba(0, 0, 0, 0.75);">μ的取值[0,1)，建议取值0.9</font>**_
+ ![](/images/df2b0695081d8c233e289c985016604f.png)

### <font style="color:rgb(79, 79, 79);">（二）自适应梯度法分类</font>
:::info
<font style="color:rgba(0, 0, 0, 0.75);">自适应梯度法通过减小震荡方向步长，增大平坦方向步长来减小震荡，加速通往谷底。</font>

:::

#### <font style="color:rgba(0, 0, 0, 0.75);">AdaGrad法与小批量梯度下降算法对比  
</font>![](/images/ab5b98af4fcb24dc66248c7051f065d9.png)
_<font style="color:rgba(0, 0, 0, 0.75);">AdaGrad法也有问题，既r随着训练数增加，也会跟着增加，这样会导致学习速率越来越小，最终变的无限小，从而无法有效更新参数，到后期r的增长对步长的调节就失效了。因此提出了下面的方法。</font>_

#### <font style="color:rgba(0, 0, 0, 0.75);">RMSProp法（是一种自适应梯度方法）  类似</font><font style="color:rgb(79, 79, 79);">Adadelta法基本思想</font><font style="color:rgba(0, 0, 0, 0.75);">  
</font>![](/images/2639e8da0b700a7aa9c4e6c8f1e71903.png)
_<font style="color:rgba(0, 0, 0, 0.75);">引入了参数γ，可以知道二阶动量其实之前所有梯度平方的一个加权均值，拆开3次累计平方梯度后表达式如下：</font>_

![image](/images/71f1848de00f96ae88b32ccc628b7706.svg)

_<font style="color:rgba(0, 0, 0, 0.75);">所以，对于adagrad算法带来的分母越来越大的问题就可以解决了。</font>_

### <font style="color:rgb(79, 79, 79);">（三）ADAM法</font><font style="color:rgb(77, 77, 77);">(Adaptive Moment Estimation自适应矩估计)</font><font style="color:rgb(79, 79, 79);">（就是动量法和自适应梯度法混合一下）</font>
<font style="color:rgb(77, 77, 77);">它是一种将动量和Adadelta或RMSprop结合起来的算法，也就引入了两个参μ，ρ</font>

+ <font style="color:rgba(0, 0, 0, 0.75);">1.见伪代码  
</font>![](/images/d3a894eb4f3e63c89f732c99924353e3.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">1.1修正偏差步骤极大缓解算法初期的冷启动问题。</font>

当算法一开始的时候

累计梯度v=0+0.1g

通过 修正偏差![image](/images/9292f848035ab46fffdeb62cdac28177.svg)

从而不会被10倍缩小

当算法10次以后会发现

修正偏差![image](/images/36d031ddd2376aeadd5cc447eb03a80f.svg)队之后的训练失去效果

+ <font style="color:rgba(0, 0, 0, 0.75);">1.2其中有</font><font style="color:#DF2A3F;">ρ、μ、Δ是超参数</font><font style="color:rgba(0, 0, 0, 0.75);">，其中</font><font style="color:#DF2A3F;">理论上可以随便设，但是已经有经验值可以使用了</font><font style="color:rgba(0, 0, 0, 0.75);">。</font>

## <font style="color:rgb(79, 79, 79);">三、权值初始化</font>
:::info
思路过程

:::

### <font style="color:rgb(77, 77, 77);">1.建议避免全0初始化，采用随机初始化。</font>
[深度学习 | (6) 关于神经网络参数初始化为全0的思考_nn.embedding初始化为全0-CSDN博客](https://blog.csdn.net/sdu_hao/article/details/104719378)

### <font style="color:rgb(77, 77, 77);">2.随即权值初始化</font>
    - <font style="color:rgba(0, 0, 0, 0.75);">2.1采用sigmoid函数作为激活函数的情况</font>
        * <font style="color:rgba(0, 0, 0, 0.75);">2.1.1权值采样自</font><font style="color:#DF2A3F;">N（0，0.01）</font><font style="color:rgba(0, 0, 0, 0.75);">的正态（高斯）分布。在采用sigmoid函数作为激活函数的情况下，由于</font><font style="color:#DF2A3F;">权值过小</font><font style="color:rgba(0, 0, 0, 0.75);">（sigmoid的导数：f(z)’ = f(z)(1 − f(z)) 的形式），</font><font style="color:#DF2A3F;">会导致信息消失</font><font style="color:rgba(0, 0, 0, 0.75);">。</font>
        * <font style="color:rgba(0, 0, 0, 0.75);">2.1.2权值采样自</font><font style="color:#DF2A3F;">N（0，1）</font><font style="color:rgba(0, 0, 0, 0.75);">的正态分布。在采用双曲正切函数作为激活函数的情况下，由于</font><font style="color:#DF2A3F;">权值过大，导致梯度过小</font><font style="color:rgba(0, 0, 0, 0.75);">，传播过程中发生</font><font style="color:#DF2A3F;">梯度消失</font><font style="color:rgba(0, 0, 0, 0.75);">。</font>

![](/images/9d2673ea76680f97bb187c2120e26206.png)

![](/images/510f55f68cd140655bd5d5effe689cea.png)

    - <font style="color:rgba(0, 0, 0, 0.75);">2.3</font>**<font style="color:#DF2A3F;">结论</font>**<font style="color:rgba(0, 0, 0, 0.75);">：</font>
        * <font style="color:rgba(0, 0, 0, 0.75);">初始化时让权值不相等，并不能保证网络能够正常工作。</font>

### <font style="color:rgb(77, 77, 77);">3.Xavier初始化-结论</font>
    - <font style="color:#DF2A3F;">有效的初始化方法</font><font style="color:rgba(0, 0, 0, 0.75);">：使网络各层的激活值和局部梯度的方差在传播过程中尽量保持一致；以保持网络中正向和反向数据流动。</font>

![](/images/0f19fbc24a798bd458ad779c2c10144f.png)

    - <font style="color:rgba(0, 0, 0, 0.75);">3.1</font><font style="color:#DF2A3F;">权值采样自N（0，</font>**<font style="color:#DF2A3F;">1/N</font>**<font style="color:#DF2A3F;">）</font><font style="color:rgba(0, 0, 0, 0.75);">的高斯分布，</font>**<font style="color:rgba(0, 0, 0, 0.75);">N</font>**<font style="color:rgba(0, 0, 0, 0.75);">为输入神经元个数。</font>



    - <font style="color:rgba(0, 0, 0, 0.75);">3.2 模拟实验的结果（图中为每层神经元输出所有结果的统计）  
</font>![](/images/972ad439a7cdad77801773f012dec9a5.png)
    - <font style="color:rgba(0, 0, 0, 0.75);">3.2Xavier初始化方法</font><font style="color:#DF2A3F;">适合sigmoid和tanh激活函数</font><font style="color:rgba(0, 0, 0, 0.75);">，但</font><font style="color:#DF2A3F;">不适合ReLU方法</font><font style="color:rgba(0, 0, 0, 0.75);">。ReLU方法的各层函数值同一如下</font>![](/images/e747353c4529a5b4f954161f2cf27207.png)
    - <font style="color:rgba(0, 0, 0, 0.75);">3.3 MSRA方法（何凯明 He方法）-一种简单改动后</font><font style="color:#DF2A3F;">适用于ReLUi或函数</font>![](/images/dc1d8fa4464505b500178d1ecacbdf47.png)
+ <font style="color:rgb(77, 77, 77);">4.Xavier初始化合理性的数学证明（SS不建议观看）  
</font>![](/images/328ee490a455b2bdac82dfde671b18c0.png)

### 总结：
![](/images/def05ea4dcb202787d210fed105be471.png)

## <font style="color:rgb(79, 79, 79);">四、批归一化（实际上做的是标准化运算）</font>
## <font style="color:rgb(79, 79, 79);">（一）归一化和标准化</font>
+ <font style="color:rgb(77, 77, 77);">1.关于标准化和归一化两个名词的具体含义</font>
+ <font style="color:rgb(77, 77, 77);">2.normalization：将一些列数据变化到某个固定区间中，通常，这个区间是[0,1]，广义的讲，可以是各种区间，比如图片通常是映射到[0,255]。</font>
+ <font style="color:rgb(77, 77, 77);">3.standardization：将数据变换为均值为0，标准差为1的分布，切记，并非一定是正态分布。（SS进行的操作是</font><font style="color:#DF2A3F;">减均值除标准差</font><font style="color:rgb(77, 77, 77);">，考研数学概率统计部分有）  
</font>![](/images/e48ca8d9c884d27e7fc38a7fb26635d7.png)
+ <font style="color:rgb(77, 77, 77);">4.通常的，将normalization翻译为归一化，将standardization翻译为标准化。但是 术语“Batch Normalization”已经广泛使用，并被深度学习社区接受。虽然名称可能不完全准确，但它的含义和具体操作已经得到了广泛的理解和应用。  </font>

## <font style="color:rgb(79, 79, 79);">（二）批归一化的操作方式</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.</font>**<font style="color:rgba(0, 0, 0, 0.75);">批归一化的方法，其操作是针对输出端的</font>**<font style="color:rgba(0, 0, 0, 0.75);">。既直接对神经元的输出进行批归一化。</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">2.具体操作中，我们要结合小批量梯度下降算法，对一批的（比如）32个输出进行减均值除方差（SS能够确定，老师讲错了，standardization应该是除标准差，ppt后面的图也是除标准差）；之后的数据输出值的分布符合0均值1方差。  
</font>![](/images/0616f687f0230bf6ac4856405b56be39.png)
    - _<font style="color:rgba(0, 0, 0, 0.75);">注意，这里的输入是BN层的输入，输出是BN层的输出</font>_

## <font style="color:rgb(79, 79, 79);">（三）将批归一化整合到神经网络中</font>
+ <font style="color:rgba(0, 0, 0, 0.75);">1.经常插入到全连接层后，非线性激活函数前。  
</font>![](/images/a7853ab561a2efc3fe9d506e9deb71db.png)
+ <font style="color:rgba(0, 0, 0, 0.75);">1.1名词解释 FC全连接神经层；BN批归一化操作；tanh激活函数</font>
