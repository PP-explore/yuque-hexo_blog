---
title: ResNet
date: '2024-06-17 20:40:59'
updated: '2025-08-21 16:18:20'
---
参考：

1. [https://blog.csdn.net/m0_74055982/article/details/137927190](https://blog.csdn.net/m0_74055982/article/details/137927190)
2. [https://blog.csdn.net/wind82465/article/details/109402167](https://blog.csdn.net/wind82465/article/details/109402167)

 ResNet在《Deep Residual Learning for Image Recognition》论文中提出，是在CVPR 2016发表的一种影响深远的网络模型，由何凯明大神团队提出来，在ImageNet的分类比赛上将网络深度直接提高到了152层，前一年夺冠的VGG只有19层。ImageNet的目标检测以碾压的优势成功夺得了当年识别和目标检测的冠军，COCO数据集的目标检测和图像分割比赛上同样碾压夺冠，可以说ResNet的出现对深度神经网络来说具有重大的历史意义。

残差神经网络的主要贡献是发现了“退化现象（Degradation）”，并针对退化现象发明了 “直连边/短连接（Shortcut connection）”，极大的消除了深度过大的神经网络训练困难问题。神经网络的“深度”首次突破了100层、最大的神经网络甚至超过了1000层。

# 一、<font style="color:rgb(77, 77, 77);">什么是退化现象？</font>
      神经网络越深的卷积层理论上可以提取更多的图像特征，但是事实结果并不是。随着CNN的发展和普及，人们发现增加神经网络的层数可以提高训练精度，但是如果只是单纯的增加网络的深度，可能会出现“梯度弥散”和“梯度爆炸”等问题。传统的解决方法则是权重的初始化(normalized initializatiton)和批标准化(batch normlization)，虽然解决了梯度问题，但是深度加深了，却带来了另外的问题，就是网络性能的退化现象，可以简单的理解为，随着训练轮数（epoch）的增加，精度到达一定程度后，就开始下架了。

![](/images/d30818f34d45ebfb3289c3f9a54921e0.png)



该图是作者在CIFAR-10上做的测试数据；左侧是训练集结果，右侧是测试集结果

        从图中可以看出错误率在20层时候是最低的，添加到了56层反而更高了。可能会有小伙伴说是不是过拟合了？其实可以看出来，如果是[动手学深度](https://www.yuque.com/yuqueyonghucfm7kr/awlus4/zl8cwmvg39eq46rv)过拟合的话，左侧的训练接在56层时候的错误率依然上升，所以并不是过拟合产生的该情况。而是由于神经网络在反向传播过程中通过链式法则不断地反向传播更新梯度，而当网络层数加深时，梯度在传播过程中会逐渐消失也就说我们所说的<font style="background-color:#FBDE28;">梯度弥散</font>。这将导致无法对前面网络层的权重进行有效的调整，网络层数越深，训练误差越高，导致训练和测试效果变差，这一现象称为<font style="background-color:#FBDE28;">退化</font>。

该现象的实质：通过多个非线性层来近似恒等映射可能是困难的(恒等映射亦称恒等函数：是一种重要的映射，对任何元素，象与原象相同的映射)。为什么恒等映射是困难的？你也可以这么理解一下，因为随着网络层数的增多，而我们优化参数需要反向传播，这一过程会不断地传播梯度，但是假设在层数较深的位置梯度很小或者很大，那么传到浅层的时候是不是就会变得非常小，那样还能优化参数吗？也就导致了无法有效地对前面网络进行有效调整。

那么，理论上本应该层次更深、效果更好的神经网络，实验结果反而不好，该怎么解决这个问题呢？很多学者都为此感到头疼，幸好RestNet姗姗赶来。

# 二、为什么深度过大而神经网络难以训练？
原因也很简单主要就是出现了梯度消失和梯度爆炸的现象，其他原因比如说是计算资源的消耗，这类原因可以通过GPU集群来解决，而过拟合的问题，可以通过采集海量的数据集和Dropout正则化可以有效解决，梯度消失和爆炸虽然也可以利用Batch Normalization解决，但根据实验发现并不如人愿望。

Batch Normalization（批量归一化）：由Google在2015年提出，是近年来DL（深度学习）领域最重要的进步之一，该方法依靠两次连续的线性变换，希望转化后的数值满足一定的特性（分布），不仅可以加快了模型的收敛速度，也一定程度缓解了特征分布较散的问题，使深度神经网络（DNN）训练更快、更稳定。（这里讲的相对松散一些，详细的内容以及原理大家可以参考其他博客），具体的过程就是通过方法将该层的特征值分布重新拉回到标准正态分布，特征值降落在激活函数对于输入较为敏感的区间，输入的小变化可导致损失函数较大的变化，使得梯度变大，避免梯度消失，同时也可加快收敛。

![](/images/9de4f383d04961ed70996c7eb54a0651.png)

梯度消失：我们知道神经网络在进行反向传播(BP算法)的时候会对参数W进行更新，梯度消失就是靠后面网络层(如layer3)能够正常的得到一个合理的偏导数，但是靠近输入层的网络层，计算的到的偏导数近乎零，W几乎无法得到更新。

原因：反向传播的时候的链式法则，越是浅层的网络，其梯度表达式可以展现出来连乘的形式，而这样如果都是小于1的，这样的话，浅层网络参数值的更新就会变得很慢，这就导致了深层网络的学习就等价于了只有后几层的浅层网络的学习了。

梯度爆炸：靠近输入层的网络层，计算的到的偏导数极其大，更新后W变成一个很大的数(爆炸)。

原因：类似于上述的原因，假如都是大于1的时候，那么浅层的网络的梯度过大，更新的参数变量也过大，所以无论是梯度消失还是爆炸都是训练过程会十分曲折的，都应该尽可能避免。![](/images/df79d9000f9d40b986641d0152cc48af.png)

# 三、残差网络解决的问题
:::info
在ResNet中，将输入直接跨层引到更深的网络层上作为输入的一部分，这个过程称之为<font style="color:#DF2A3F;">恒等映射</font>。恒等映射使得数据可以在网络中实现跨层流动。论文中提到：

Identity shortcut connections add neither extra parameter nor computational complexity

Identity shortcut connections既不会增加额外的更多的参数，也不会增加网络计算的复杂度。

:::

神经网络越来越深的时候，反传回来的梯度之间的相关性会越来越差，最后接近白噪声。因为我们知道图像具有局部相关性，可以认为梯度也应该具备类似的相关性，这样更新的梯度才有意义，如果梯度接近白噪声，那梯度更新可能根本就是在做随机扰动。基于这种退化问题，作者通过浅层网络等同映射 （恒等映射，identity mapping）扩展到深层模型  ，结果深层模型并没有比浅层网络有等同或更低的错误率，推断退化问题可能是因为深层的网络并不是那么好训练，也就是求解器很难去利用多层网络拟合同等函数。

如果深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那当前要解决的就是学习恒等映射函数了。这段话也是可以这样理解一下，<font style="color:#DF2A3F;">如果更深层次的网络都能够学习到恒等映射，那么准确率起码不会降低，所以这正是残差网络的解决的问题</font>。

# 四、ResNet网络解析
:::info
<font style="color:rgb(79, 79, 79);">Plain Networks（平原网络）</font>

平原网络是作者对残差网络改进之前的简单堆叠多个layers的称呼，是和残差网络（Residual Networks）相对的一个概念。

:::

       传统的平原网络如下图所示，这是一个普通的、两层的卷积+激活。



                                     ![](/images/c0807a2a4cc53e0d59aa0b8caf356905.png)                               



        经过两层卷积+一个激活，我们假定它输出为H(x)。与传统的网络结构相比，ResNet增加了“短路”连接(shortcut connection)或称为跳跃连接(skip connection) ，如下图所示：

:::info
<font style="color:rgb(79, 79, 79);">Shortcut Connections（快捷连接）</font>

<font style="color:rgb(77, 77, 77);">在ResNet网络结构图中，shortcut connections就是identity mapping在网络结构图中的具体实现，也就是右侧弯曲的那条线。它既不会增加额外的更多的参数，也不会增加网络计算的复杂度。</font>

:::

![](/images/3810751dbbec766ca22ea641a7e822c5.png)

        它添加了一个短路连接到第二层激活函数之前。那么激活函数的输入就由原来的输出H(x)=F(x)变为了H(x)=F(x)+x。在RestNet中，这种输出=输入的操作成为恒等映射。那么，上图中的identity其实功能也是恒等映射。

        那么这么做的好处是什么呢？在深度神经网络中，随着训练过程中反向传播权重参数的更新，网络中某些卷积层已经达到最优解了，其实此时这些层的输入输出都是一样的，已经没有训练的必要。但实际训练过程中，我们是很难将权重参数训练为绝对0误差的，但是这种情况已经是最优解了，其实对这些层的训练过程是可以抛弃的，即此时可以设F(x)=0，那么这时的输出为H(x)=x就是最优输出。

        在传统平原网络中，即未加入identity之前，如果网络训练已经达到最优解了，那么随着网络继续训练、权重参数的更新，有可能将已经达到最优解的权重参数继续更新为误差更多的值。但随着identity的加入，在达到最优解的时候直接通过F(x)=x,那么权重参数可以达到至少不会比之前训练效果差的目的，并且可以加快网络收敛。

       在解决梯度弥散的问题上，其实可以通过如下的公式分析：

                                                            ![](/images/9443d7c2221ef1ee14445b4b3d6bd297.png)

        上面的公式中，现有网络的某个深层的卷积层，表示某个残差的输入层，可以看出在残差网络中，下面的层次残差的块的输出都可以由上面的某一层确定。

在反向传播中，残差网络的梯度公式求带后如下所示：



                                                   ![](/images/2f479430c32eb99bd03a9ced5250bbe4.png)



      可以看出，反向传播的梯度由2项组成，对x的直接映射，梯度为1；为多层普通神经网络映射上的结果。

即使新增的多层神经网络的梯度为0时残差网络的梯度更新多了一个“1”！就是我们之前定义的identity这条路径，正是由于多了这条捷径，来自深层的梯度能直接畅通无阻地通过，去到上一层，使得浅层的网络层参数等到有效的训练，避免了梯度消失问题。

![](/images/fa6c687103d73b6aa39c58345d1f074d.jpeg)

 那么，现在我们可以展示一下残差网络整体模型结构。将平原网络修改为残差网络后的网络结构如下所示：

         [https://zhuanlan.zhihu.com/p/67860570](https://zhuanlan.zhihu.com/p/67860570)        

        从上图中可以看到每两个卷积层就添加了一个“短路”连接。实现就是我们的直连identity shortcuts。但是有些网络中，F(x)和x的维度不一致，此时我们需要做一下为杜转换，上图的虚线就是我们带有维度操作的残差块，如下所示就是带有卷积操作的残差块。

  这种情况下，我们原有残差公式由H(x)=F(x)+x，变为H(x)=F(x)+*x。

       以上就是残差网络的详细解释。对于50层以上的深度网络模型，何凯明团队还设计了多用1×1的卷积层，减少了网络参数，如下图所示：

       上图可以看出，针对层次较多的网络，将原有的两个3×3卷积变为了1×1、3×3和1×1的卷积。 在ResNet50、ResNet101和ResNet152中都是用了该结构设计。

       最后附上论文中残差网络的实验结果。



    



