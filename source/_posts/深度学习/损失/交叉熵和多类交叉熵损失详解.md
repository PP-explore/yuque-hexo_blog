---
title: 交叉熵和多类交叉熵损失详解
date: '2025-10-15 15:26:18'
updated: '2025-10-15 17:01:33'
categories:
  - 人工智能
tags:
  - 深度学习
  - 损失函数
cover: /images/custom-cover.jpg
recommend: true
---
交叉熵损失度量了模型输出的概率分布pi和真实标签分布qi之间的差异。交叉熵越小，表示两个分布越接近，即预测的类别概率分布越接近真实标签。

信息论 意义下的  的交叉熵定义（单个样本的）：

![](/images/81b482a563c0f4a984cc735202253c7b.png)

在二分类任务中：

对于一个样本，无论他是正样本还是负样本，交叉熵损失都会计算该样本的正标签类别的预测分数的损失

![](/images/3e0224db7af5a0d3d0e9645970556aa2.png)

<font style="color:rgb(255, 255, 255);background-color:rgb(29, 31, 32);">真实标签y通常</font>**<font style="color:rgb(255, 255, 255);background-color:rgb(29, 31, 32);">取值为 0 或 1</font>**<font style="color:rgb(255, 255, 255);background-color:rgb(29, 31, 32);">。</font>

<font style="color:rgb(255, 255, 255);background-color:rgb(29, 31, 32);">预测值 y_是模型输出的</font>**<font style="color:rgb(255, 255, 255);background-color:rgb(29, 31, 32);">类别 1 的概率</font>**

在多分类任务中：

   多分类没有正负样本之分，使用的是one-hot编码

![](/images/53f521fb810d22959a36093c74cc72c0.png)

+ pi是真实分布（one-hot，比如真实类别是2 → p = [0, 0, 1]）
+ qi是模型预测分布（softmax输出）

例如真实类别是「狗」，则p=[0,1,0]，模型预测 q=[0.2,0.5,0.3]  
则：H(p,q)=−(0log⁡0.2+1log⁡0.5+0log⁡0.3)=−log⁡(0.5)



我们在训练模型时更新参数（反向传播）时，是希望模型在“整体上”表现更好，对同一batch中多个样本计算交叉熵。

所以**深度学习训练中通常**使用的是「平均交叉熵损失」：  

![](/images/5f86b4fa5e1aee79e23061748a46c50e.png)

>  取均值是为了不让某个样本单独主导整个梯度方向，而是让每个样本的损失对总梯度有「均等」贡献  
>

验证和对比

```yaml
import torch

# Softmax函数
def soft_max(x):
    x_exp = torch.exp(x)
    partition = x_exp.sum(1, keepdim=True)
    return x_exp / partition

# 手动计算交叉熵损失
def cross_entropy(y, y_hat_softmax):
    # 取出真实类别对应的预测概率
    p = y_hat_softmax[range(len(y_hat_softmax)), y]  #python语法   ：【（x,x），（y,y）】
    # 取log再取负号求均值
    loss = -torch.log(p)
    return loss.mean()

# === 测试部分 ===
if __name__ == "__main__":
    # 模拟样本
    y = torch.tensor([0, 2])   # 真实类别标签
    y_hat = torch.tensor([[0.1, 0.3, 0.6], 
                          [0.3, 0.2, 0.5]])  # 网络输出（未经过softmax）

    # softmax概率
    y_hat_softmax = soft_max(y_hat)
    print("softmax输出：\n", y_hat_softmax)

    # 手动计算交叉熵
    manual_loss = cross_entropy(y, y_hat_softmax)
    print('手动计算的平均交叉熵损失:', manual_loss.item())

    # 使用PyTorch内置的CrossEntropyLoss（注意它内部会自动做log_softmax）
    cr_loss = torch.nn.CrossEntropyLoss(reduction="mean")
    builtin_loss = cr_loss(y_hat, y)
    print('torch内置CrossEntropyLoss计算的损失:', builtin_loss.item())

```
