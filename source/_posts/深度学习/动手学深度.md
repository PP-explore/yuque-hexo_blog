---
title: 动手学深度
date: '2024-05-15 23:06:18'
updated: '2025-08-21 16:18:20'
---
![](/images/8a2fa7f28f9be89d51f05540ead68a25.png)

[Git 详细安装教程（详解 Git 安装过程的每一个步骤）_git安装-CSDN博客](https://blog.csdn.net/mukes/article/details/115693833)

# torch的维度理解
dd=[[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]]

对于这个dd，0维度就是最外围的矩形框

1维度就是次外围的矩形框

# 范数
:::info
<font style="color:rgb(77, 77, 77);">在数学上，范数包括向量范数和矩阵范数，向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小。</font>

:::

<font style="color:rgb(77, 77, 77);">一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；对于矩阵范数，学过线性代数，我们知道，通过运算AX=B，可以将向量X变化为B，矩阵范数就是来度量这个变化大小的。</font>![](/images/19d59b574e6e8ee3f63b65a9a15c0f2a.png)

#### <font style="color:rgb(79, 79, 79);">扩展一点：</font>
<font style="color:rgb(77, 77, 77);">使用机器学习方法解决实际问题时，我们通常要用L1或L2范数做正则化（regularization），从而限制权值大小，减少过拟合风险。特别是在使用梯度下降来做目标函数优化时，</font>

[什么是范数（norm）？以及L1,L2范数的简单介绍_l1 norm-CSDN博客](https://blog.csdn.net/qq_37466121/article/details/87855185)

# torch代码基础计算


```csharp
检查torch是否能使用cuda
torch.cuda.is_available()

主要自学工具函数
dir（）和help（）
对于help（）反回 的参数格式为__xxxx__则表明这个属性或者参数为不可素以修改的



矩阵的哈德曼积（按元素进行乘积）
A*B
矩阵与标量的运算（按元素进行）
a+X，a*X
降维度
A.sum()计算所有元素总和
A.sum(axis=)计算沿着某个轴向通过求和降低维度//可以看作降维度，从外围开始降
A.sum(axis=[0, 1])  # 计算多个轴向降维度
求平均值
A.mean(), A.sum() / A.numel() //元素总和除以元素个数
A.mean(axis=0), A.sum(axis=0) / A.shape[0]//也可以按照某一个轴求某一个州的平均值

非降维按轴求和
sum_A = A.sum(axis=1, keepdims=True)

累加求和（不降低维数）cumulative sum
A.cumsum(axis=0)//递进式求和，沿轴向依次将上方元素加给下方元素

向量的点积
torch.dot(x,y)
矩阵向量积（实质上就是矩阵成乘法，只不过一个矩阵为向量）matrix-vector product
torch.mv()
矩阵乘法 matrix-matrix product
torch.mm()

向量范数
torch.abs(v).sum()  //L1范数并没有专有的函数
torch.norm(v)       //L2范数为向量元素平方和的平方根

矩阵范数
F范数  torch.norm(m)  //F范数为矩阵元素的平方和的平方根
```

# 分母布局和分子布局
[矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇）](https://zhuanlan.zhihu.com/p/263777564)

:::info
关键点：

1. 知道什么是梯度以及梯度的计算
2. 分子布局与分母布局的差别，参考上面的链接，解释的很清楚
3. 矩阵向量化以及转置，参考上方链接

:::

![](/images/d8669eed89fb8628d34295b0a3102ad6.png)

## $ 

       $标量方程对向量的导数（梯度）：
:::info
标量方程指的是f(y_1,y_2,....，y_n),方程有n个自变量，同时f运算后是一个标量。

     那么求解偏导数时，则是对各自变量求偏导组成方程组

:::

    

![](/images/d89e103feef5aff3137966b65144f5d0.png)

那么定义

![](/images/9c38943a5d0e7994b20436b9a8f5e894.png)

将自变量组成向量 ![image](/images/45e48141800380974356282275150605.svg)为mx1,则得到行数与分母相同  称为分母布局

![](/images/b0073aebdc97acc0fc5ba8a75f7a9385.png)

那么写成行数与分子相同这叫分子布局



## 向量方程对向量的导数（梯度）
![](/images/9f37f805f74532239f2b07139088abd8.png)

# ![](/images/88bc38f51bf8c8e73aa44000c3b7044a.png)
##  记忆小技巧：
+ 可以把「标量对向量」和「向量对标量」求导中的**标量看作是一维的行向量**，而**向量则为一般理解的列向量**，则分母/分子布局就表示求导后的向量的布局是跟求导式的分母还是分子保持一致。
1. 规则1：「标量对向量」求导：分母布局结果为列向量，分子布局结果为行向量。
2. 规则2：「向量对标量」求导：分母布局结果为行向量，分子布局结果为列向量。
+ 对于「向量对向量」求导：既可以看作是分子向量中的每个标量元素对分母向量求导，也可以看作是分母向量对分子向量中的每个标量元素求导，然后便可以使用上一条记忆方法。最终将每个求导的结果向量拼接成一个矩阵，即得到最终的分母/分子布局结果。

<font style="color:rgb(24, 25, 28);">分母布局时，梯度结果第一个维度按分母的第一个维度</font>  


##  常用特例1： ![image](/images/05e560aa246d211d0d9c678280c8f6ea.svg)
![](/images/d180d3c7a711f4f3977b4073e340849e.png)

实际上这个例子就是对向量乘以一个系数矩阵后得到的向量方程求偏导，由于是分母布局则为A^T，如是分子布局则为A

##  常用特例2： 


![](/images/23e53fc8ce468247012fe3a5a93fc19e.png)

![](/images/c9f54a2dc557507363ec14cd2a2d6827.png)

## 各类方程的导数：
![](/images/08d8108e442dfec376c6e8ed4848869d.png)

总结：（我的理解有错，可以按照规则1，2推断）

上图，李沐为分子布局。按照分子布局的规则。

1. 向量y方程对向量x求导：

先展开x（先展开y也可以）因为分子y为m，1，分子第一个维度不变，第二个维度1变为n，变为（m，n）。

2. 矩阵Y方程对举证X求导：

先展开Y（先展开X也可以），分子布局，则第一个维度为Y的m，每一项变为向量y方程对矩阵X求导。

继续展开y则为m，l，每一项变为标量y方程对举证X求导。

继续展开，则因为分子布局为（m,l,1），此时每一个项都是![image](/images/87d9ff038b59b9c15eb5eaad0d6ad262.svg),每一项都是标量方程对举证的求导，继续拆开则变为（m,l,k,n）.-----这里可以看成更高维度拆分，或者将第三维度当作第一维度继续进行拆分

# 向量的链式法则
![](/images/30b4e012551848d6f56aae93f2dca83d.png)

拓展到向量：则直接考虑矩阵的乘法即可。

由于神经网络有多达几百层，所以我们需要自动求导

## 自动求导
定义：![](/images/e37b1dcb5390d39d7726c121a5896cd8.png)

自动求导涉及计算图的概念，pytorch不需要理解计算图，但是需要他的原理，以便使用其他的框架

![](/images/f1474abb1de0bfff32a7ea0ebee63dee.png)

### 计算图 
![](/images/9571ffbf2dd221cdfcfe17e518ae895c.png)

![](/images/ce49f2e04a134655a6810629b7d5d4de.png)

















### 自动求导的俩种模式
### ![](/images/e76553ecad80a77ac9d25392259039d9.png)
 ![](/images/b0e2a7a410921a0d0c5eeea9b81395d7.png)

理解正向和反向：

首先对于神经网络，我们会先输入训练数据（数值），正向计算出该数值沿着网络计算出的结果，再反向计算梯度，正向计算的时候就会保留所有的中间变量。计算梯度的时候就因为正向输入数据时就已经保存了中间计算的每一个结果，就可以直接代入中间计算的结果到导函数中得到导数。

根据链式法则最终的梯度就是各个操作子导数的乘积

![](/images/852cfcfc8b7281e6dbb162dc629fb6d2.png)

![](/images/ec5d8a140f31a9bc8294678a10366c19.png)

因此才会出现深度神经网络吃GPU的情况，应为每一次求梯度都要保存之前的结果

# <font style="color:rgb(79, 79, 79);">过拟合</font>
## <font style="color:rgb(79, 79, 79);">一、什么是过拟合？</font>
<font style="color:rgb(77, 77, 77);">过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，模型在训练集上表现很好，但在测试集上却表现很差。模型对训练集"死记硬背"（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，泛化能力差。</font>![](/images/aa3898e3aa5915c5342ac846a34a8a47.png)

## 二、过拟合出现原因
1. 建模样本选取有误，如样本数量太少，选样方法错误，样本标签错误等，导致选取的样本数据不足以代表预定的分类规则
2. 样本噪音干扰过大，使得机器将部分噪音认为是特征从而扰乱了预设的分类规则
3. 假设的模型无法合理存在，或者说是假设成立的条件实际并不成立
4. 参数太多，模型复杂度过高
5. 对于决策树模型，如果我们对于其生长没有合理的限制，其自由生长有可能使节点只包含单纯的事件数据(event)或非事件数据(no event)，使其虽然可以完美匹配（拟合）训练数据，但是无法适应其他数据集
6. 对于神经网络模型：

a)对样本数据可能存在分类决策面不唯一，随着学习的进行,，BP算法使权值可能收敛过于复杂的决策面

b)权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征

# 激活函数
[激活函数的理解及其工作原理_metaaconc-CSDN博客](https://blog.csdn.net/a699669/article/details/122128940)

:::color1
**<font style="color:rgb(77, 77, 77);">神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数</font>**<font style="color:rgb(77, 77, 77);">）</font>

:::

## 为什么需要激活函数？
经过上述的讨论也可以得出激活函数的大概用途（引用自tyhj_sf）：

如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数

## 各类激活函数
[https://blog.csdn.net/jsk_learner/article/details/102822001](https://blog.csdn.net/jsk_learner/article/details/102822001)

### ReLU
[https://blog.csdn.net/weixin_41929524/article/details/112253138](https://blog.csdn.net/weixin_41929524/article/details/112253138)

ReLU，全称为：Rectified Linear Unit 整流线性单元，是一种[人工神经网络](https://so.csdn.net/so/search?q=%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=1001.2101.3001.7020)中常用的激活函数，通常意义下，其指代数学中的斜坡函数，即f(x)=max(0,x)

对应的函数图像如下所示：  
![](/images/fb70de77b3c09ce7e38832999ff26bd9.png)而在神经网络中，ReLU函数作为神经元的激活函数，为神经元在线性变换 **<font style="color:rgb(77, 77, 77);">w</font>**<font style="color:rgb(77, 77, 77);">T</font>**<font style="color:rgb(77, 77, 77);">x</font>**<font style="color:rgb(77, 77, 77);">+b </font> 之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量 x xx，使用ReLU函数的神经元会输出<font style="color:rgb(77, 77, 77);">max(0,</font>**<font style="color:rgb(77, 77, 77);">w</font>**<font style="color:rgb(77, 77, 77);">T</font>**<font style="color:rgb(77, 77, 77);">x</font>**<font style="color:rgb(77, 77, 77);">+b)</font>

至下一层神经元或作为整个神经网络的输出（取决现神经元在网络结构中所处位置）。

# Layer Normalization<font style="color:rgb(34, 34, 38);">解析</font>
[https://blog.csdn.net/qq_37541097/article/details/117653177](https://blog.csdn.net/qq_37541097/article/details/117653177)





# 代码理解  
pytorch：
###  argparse 标准库
[【Python进阶】argparse库基础用法全总结：高效脚本参数解析 | 参数类型使用代码-CSDN博客](https://blog.csdn.net/Q52099999/article/details/137028697)

```python
class argparse.ArgumentParser(prog=None, usage=None, description=None, epilog=None, parents=[], formatter_class=argparse.HelpFormatter, prefix_chars='-', fromfile_prefix_chars=None, argument_default=None, conflict_handler='error', add_help=True, allow_abbrev=True, exit_on_error=True)¶

创建一个新的 ArgumentParser 对象
部分比较有用的参数：
prog - 程序的名称 (默认值: os.path.basename(sys.argv[0]))
usage - 描述程序用途的字符串（默认值：从添加到解析器的参数生成）
description - 要在参数帮助信息之前显示的文本（默认：无文本）

ArgumentParser.add_argument(name or flags..., *[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest][, deprecated])¶

add_argument() 方法必须知道是要接收一个可选参数，如 -f 或 --foo，还是一个位置参数，如由文件名组成的列表。 因此首先传递给 add_argument() 的参数必须是一组旗标，或一个简单的参数名称。
parser.add_argument('-f', '--foo')  #可选参数
parser.add_argument('bar') #位置参数

action 命名参数指定了这个命令行参数应当如何处理。供应的动作有：
1.'store' - 这用于存储参数的值。 这是'默认的动作'。
2.'store_const' - 存储由 const 关键字参数指定的值；请注意 const 关键字参数默认为 None。 'store_const' 动作最常被用于指定某类旗标的'可选参数'
3.parser.add_argument('--foo', action='store_const', const=42)
'append' - 存储一个列表，并将每个参数值添加到该列表。


```

```python
import torch
#torch.manual_seed(0) 是用来设置 PyTorch 的随机数生成器的种子，从而确保程序的随机行为是可重复的。
torch.manual_seed(0)
print(torch.randn(3))  # 输出：tensor([ 1.5410, -0.2934, -2.1788])
torch.manual_seed(0)
print(torch.randn(3))  # 输出：tensor([ 1.5410, -0.2934, -2.1788])


#device = torch.device(f"cuda:{rank}") 是用来指定当前进程要使用的 GPU 设备
#torch.cuda.set_device(device)在设置 device 之后，通常需要将当前 GPU 绑定到指定设备上。

device = torch.device("cpu")       # 使用 CPU
device = torch.device("cuda")      # 使用默认的 GPU
device = torch.device("cuda:0")    # 使用第 0 个 GPU
device = torch.device("cuda:1")    # 使用第 1 个 GPU
在分布式训练中，每个进程通常与一个 GPU 绑定，rank 表示当前进程的编号

```

## DDP分布式训练：
+ **Data Parallelism（数据并行）**：将数据划分到多个设备上，每个设备独立训练，最后汇总梯度。
+ **DDP 的核心原理**：在每个 GPU 上运行一份模型，每个 GPU 负责一部分数据。训练过程中，所有 GPU 同步计算得到的梯度。
+ **通信方式**：
    - 在单机多卡环境下，通常使用 **NCCL**。
    - 在多机多卡环境下，需要设置主节点的地址和端口进行通信。

---

### **2. 使用 DDP 的要求**
1. **DDP 模块**：需要使用 `torch.nn.parallel.DistributedDataParallel` 包裹模型。
2. **初始化分布式环境**：调用 `torch.distributed.init_process_group`。
3. **为每个 GPU 设置独立的进程**：通常使用 `torch.multiprocessing.spawn` 或直接运行多个进程。
4. **数据加载器**：使用 `torch.utils.data.distributed.DistributedSampler` 确保每个 GPU 处理不同的数据子集。

---

### **3. 实现步骤**
#### **1. 环境配置**
+ 确保 CUDA 和 NCCL 正常安装。
+ 检查是否支持多 GPU：

#### **2. 基础代码示例**
以下是一个使用 DDP 的单机多卡训练代码示例：

```python

import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from torchvision import datasets, transforms
import torch.nn as nn
import torch.optim as optim

# 模型定义（简单的线性模型）
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(28 * 28, 10)

    def forward(self, x):
        return self.fc(x.view(x.size(0), -1))

def setup(rank, world_size):
    """
    初始化分布式训练环境
    :param rank: 当前进程的 rank
    :param world_size: 总进程数
    """
    os.environ['MASTER_ADDR'] = '127.0.0.1'  # 主节点地址
    os.environ['MASTER_PORT'] = '12355'     # 主节点端口
    dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)

def cleanup():
    """销毁分布式训练环境"""
    dist.destroy_process_group()

def train(rank, world_size):
    """
    分布式训练逻辑
    :param rank: 当前进程的 rank
    :param world_size: 总进程数
    """
    print(f"Running DDP on rank {rank}.")
    setup(rank, world_size)

    # 设置当前设备
    torch.manual_seed(0) #设置 PyTorch 的随机数生成器的种子，从而确保程序的随机行为是可重复的。
    device = torch.device(f"cuda:{rank}")
    torch.cuda.set_device(device)

    # 数据集和分布式采样器
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    dataset = datasets.MNIST(root="./data", train=True, transform=transform, download=True)
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)
    dataloader = DataLoader(dataset, batch_size=64, sampler=sampler)

    # 模型、损失函数和优化器
    model = SimpleModel().to(device)
    model = DDP(model, device_ids=[rank])  # 使用 DDP 包裹模型
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01)

    # 训练循环
    for epoch in range(5):
        sampler.set_epoch(epoch)  # 确保每个 epoch 的数据划分不同
        for batch_idx, (data, target) in enumerate(dataloader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            if batch_idx % 100 == 0:
                print(f"Rank {rank}, Epoch {epoch}, Batch {batch_idx}, Loss {loss.item()}")

    cleanup()

if __name__ == "__main__":
    world_size = torch.cuda.device_count()  # 获取 GPU 数量
    torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size, join=True)
```

---

### **4. 代码解析**
#### **1. 初始化分布式训练环境**
+ `torch.distributed.init_process_group`：初始化进程组，指定 backend（如 NCCL）以及通信地址和端口。

#### **2. 数据并行性**
+ **分布式采样器**：`DistributedSampler` 确保数据被均匀分布到每个 GPU。

#### **3. 模型并行**
+ `DistributedDataParallel`：封装模型，并指定 `device_ids`。

#### **4. 进程管理**
+ **多进程启动**：通过 `torch.multiprocessing.spawn` 创建多个进程，每个进程负责一个 GPU。

---

### **5. 优化建议**
1. **使用 **`**find_unused_parameters=True**`： 如果部分参数没有被用到，必须设置该参数以避免报错：

```plain
python


复制代码
model = DDP(model, device_ids=[rank], find_unused_parameters=True)
```

2. **减少显存消耗**：
    - 启用 **混合精度训练（AMP）**：

```plain
python


复制代码
scaler = torch.cuda.amp.GradScaler()
with torch.cuda.amp.autocast():
    output = model(data)
    loss = criterion(output, target)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

3. **多机多卡扩展**：
    - 修改 `--nnodes` 和 `--node_rank`。
    - 确保主节点地址可访问，并设置正确的 `MASTER_ADDR` 和 `MASTER_PORT`。

---

### **6. 小结**
通过 DDP，可以高效地利用多 GPU 或多节点资源进行分布式训练。掌握以下关键点：

1. 初始化分布式环境。
2. 使用 `DistributedSampler` 进行数据划分。
3. 使用 `DistributedDataParallel` 包裹模型。
4. 使用多进程启动训练。

原理：[实操教程 | GPU多卡并行训练总结（以pytorch为例）](https://mp.weixin.qq.com/s/IdVe9gy8xRyhuWCyanidmw)

torch.nn.parallel.DistributedDataParallel

```python
import argparse
import 
#导入DDP模块
from torch.nn.parallel import DistributedDataParallel as DDP

#参数需要包括以下
parser = argparse.ArgumentParser()
parser.add_argument("--save_dir", default='')
parser.add_argument("--local_rank", default=-1)
parser.add_argument("--world_size", default=1)
args = parser.parse_args()

# 初始化后端

# world_size 指的是总的并行进程数目
# 比如16张卡单卡单进程 就是 16
# 但是如果是8卡单进程 就是 1
# 等到连接的进程数等于world_size，程序才会继续运行
torch.distributed.init_process_group(backend='nccl',
                                     world_size=ws,
                                     init_method='env://')

torch.cuda.set_device(args.local_rank)

device = torch.device(f'cuda:{args.local_rank}')

model = nn.Linear(2,3).to(device)

# train dataset
# train_sampler
# train_loader

# 初始化 DDP，这里我们通过规定 device_id 用了单卡单进程
# 实际上根据我们前面对 parallel_apply 的解读，DDP 也支持一个进程控制多个线程利用多卡
model = DDP(model,
            device_ids=[args.local_rank],
            output_device=args.local_rank).to(device)


# 保存模型 
if torch.distributed.get_rank() == 0:
    torch.save(model.module.state_dict(),
               'results/%s/model.pth' % args.save_dir)
```



